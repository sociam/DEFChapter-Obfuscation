% PLEASE USE THIS FILE AS A TEMPLATE FOR THE PUBLICATION 
% Check file IOS-Book-Article.tex
%

\documentclass{IOS-Book-Article}     %[seceqn,secfloat,secthm]
%\usepackage{mathptmx}
%\usepackage[T1]{fontenc}
%\usepackage{times}%
%
%%%%%%%%%%% Put your definitions here
\usepackage{paralist}
\usepackage{graphicx}
\usepackage{color}
\usepackage{url}


%Make figures easier
\newcommand{\fig}[3][0.9]{
\begin{figure}[tp]
\begin{center}
\includegraphics[width=#1\textwidth]{figures/#2}
\caption{#3}
\label{fig:#2}
\end{center}
\end{figure}
}

\newcommand{\tbox}[3][red]{{
\color{#1}\noindent{
   \fbox{\scriptsize{ {\bf #2} \textsl{#3}}}
   \vspace{2pt}
}
}}


\definecolor{darkgreen}{rgb}{0,0.4,0}
\newcommand{\todo}[1]{\tbox{TODO:}{#1}}

%%%%%%%%%%% End of definitions
\begin{document}
\begin{frontmatter}          % The preamble begins here.
%
%\pretitle{}
\title{Social Palimpsests - clouding the lens of the personal panopticon}

% \subtitle{\ldots or ``Suck my tailpipe'' - how I learned to stop worrying and toxify my exhaust data}

% emax: not being a hater, but this subtitle is seriously not safe for life -- can we figure out something a bit with more class and sass and less... penis? :) 

% Two or more authors:
\author[A]{\fnms{Dave} \snm{Murray-Rust}},
\author[B]{\fnms{Max} \snm{Van Kleek}},
\author[B]{\fnms{Laura} \snm{Dragan}},
\author[B]{\fnms{Nigel} \snm{Shadbolt}}

\runningauthor{Murray-Rust et. al.}
\address[A]{Centre for Intelligent Systems and Applications,
Department of Informatics, University of Edinburgh}
\address[B]{Web and Internet Science Research Group, Electronics and Computer Science, University of Southampton}

\begin{abstract}
The use of personal data has incredible potential to benefit both society and
individuals, through increased understanding of behaviour, communication and
support for emerging forms of socialisation and connectedness. However, there
are risks associated with disclosing personal information, and present systems
show a systematic asymmetry between the subjects of the data and those who
control and manage the way that data is propagated and used. In this chapter, we
explore a set of obfuscation techniques for ameliorating the tension between
the benefits of sharing, and a distrust of those with whom we share our data.
\end{abstract}

\begin{keyword}
Obfuscation; Data politics; Personal Data Stores; Social Machines; Pants-on-fire;
\end{keyword}

\end{frontmatter}

%%%%%%%%%%% The article body starts:

\section*{Introduction}

%% maxxie wrote some stuff here

The use and development of tools for protecting people's privacy online used to
be framed as supporting the activities of criminals, miscreants and even terrorists.  
However, as increasingly sophisticated methods for tracking and modelling individuals
have become pervasively deployed in order to extract economic value from them, 
the old ``I have nothing to hide; therefore I should not care'' argument has fallen apart. 
Previously theoretical arguments of what \emph{might} happen if one is pervasively tracked have been replaced by the tangible reality of hundreds, or thousands of separate information brokers 
(some visible, others invisible), grappling with one another to build the most 
complete picture of each and every individual's life, so that such models 
can be used to precisely target, and behaviourally manipulate individuals,
often in ways too subtle to be perceived (e.g. \cite{Kramer17062014}). 

The sophisticated methods being used to track and pinpoint details of individuals 
has not been met with a corresponding increase in public education to spread awareness
of how people are being tracked, nor with a proliferation of counter-tracking and 
counter-surveillance tools that can be readily used by end-user citizens to defend 
themselves.  As a result, a great asymmetric divide has grown between 
those brokers that track and citizens who are being monitored and manipulated, 
and whose individual autonomy is being threatened.  

% 
% Such asymmetry means that privacy protection can no longer be merely a concern 
% for a minority with peculiar sensitivities towards being seen or
% exposed, but instead, as necessary preserving
% individual autonomy and well-being, in an economy where personal data has become
% the most precious digital commodity. 

% has not been met with a corresponding increase in counter-tracking and counter-surveillance tools

% The ``I have nothing to hide; therefore I should not care'' argument \cite{} used often in defense of ignoring
% privacy in the early days of targeted advertising and the Web still re-surfaces
% today. But as theoretical arguments of what \emph{may} become have been replaced
% by tangible examples of what has actually has, this, among other arguments have
% become thoroughly dismantled.  The sophisticated methods being used to track and
% pinpoint details of individuals has not been met with a corresponding increase
% in counter-tracking and counter-surveillance tools; as a result, people are
% being subject to an unprecedented barrage of targeted attempts to behaviourally
% manipulate them - often in ways too subtle for them to perceive (e.g.
% \cite{facebookstudy}).           

% The use and development of tools for protecting people's privacy online have 
% been framed as methods for empowering and enabling criminals, miscreants 
% and even terrorists to more easily conduct their misdeeds against humanity
% without consequence.  The ``I have nothing to hide; therefore I should not care'' argument \cite{} used often in defense of ignoring
% privacy in the early days of targeted advertising and the Web still re-surfaces
% today. But as theoretical arguments of what \emph{may} become have been replaced
% by tangible examples of what has actually has, this, among other arguments have
% become thoroughly dismantled.  The sophisticated methods being used to track and
% pinpoint details of individuals has not been met with a corresponding increase
% in counter-tracking and counter-surveillance tools; as a result, people are
% being subject to an unprecedented barrage of targeted attempts to behaviourally
% manipulate them - often in ways too subtle for them to perceive (e.g.
% \cite{facebookstudy}).           


Bentham's thought experiment of the Panopticon, developed by Foucault, describes
a prison in which inmates lives are constantly surveilled as a means of
discipline and exertion of control.  In the world of technologically driven 
data collection which we inhabit, this has effects both on people's behaviour as
they internalise the fact that they are surveilled, and on the way in which they
are treated, and \emph{socially sorted} by the gears of our algorithmic
society\cite{simon2005Panopticism}.

In this chapter, we are interested in practices which can cloud the lenses of the observers, to
choose how we are seen, and to help regain some control of
the manner in which our lives are surveilled, both socially and otherwise.

% Bruce Schneier: ``privacy is not about something to hide its about autonomy and control; when you are observed you cannot be in control''

We discuss and analyse methods of privacy protection that
advance beyond the current state of anonymisation tools that merely obscure the tracks
of individuals, towards those that employ methods borrowed from
information warfare~\cite{gursestranslating,lyon2007surveillance}, in order to allow individuals to regain autonomy
from unsolicited tracking and behavioural control.  We first discuss the rise of 
personal data economy, followed by a survey of lying and falsification in context-aware 
systems, current anonymisation and privacy tools. This is followed by an overview of
strategies for obfuscation, some of which are currently implemented in either
mainstream tools or proof of concept studies,  and some of which are
speculative, future possibilities.

\section{Background}

\subsection{The rise of personal data and services reliant on it, and relation
to surveillance}

As we pass through the digitally augmented world that we collectively inhabit,
the set of actions with the potential to produce data grows year on year.
Portions of this outpouring are kept and stored as
\emph{capta}, from \emph{capere}: to keep~\cite{dodge2005codes}. From
using an access card to unlock a door at the workplace, right down to tracking
individual footfalls, pervasive digital systems illuminate and annotate our
physical activity. Accreting around this body of physical observation is an
expanding sphere of mental observation and analysis. This can take the form of
active practises around recording mental states, such as journalling, but it can
also include computational inference, where frequency of posting on social
networks becomes an adjunct metric for connectedness, and search terms 
indicators of intent. As such, the modes of collecting this information can
range from explicit, user initiated submission of data, through consensual
background recording, to invisible, asymmetric electronic surveillance.

Increasingly, in order to utilise services, we must provide our data to third
parties. This ranges from mobile phone numbers being required for Yahoo
accounts, to location data being shared with Foursquare or Grindr, to the NHS
adding personal health information to centralised databases. 
The pervasiveness of computationally mediated action and interaction in modern
life means that for much of this data ``refusal is not a practical option, as
data collection is an inherent condition of many essential societal
transactions''\cite{brunton2011vernacular}. 

This leads us to introduce the term 
\emph{fiat data}---when an
organisation uses its position to demand (by fiat) the disclosure of certain
information in return for use of its services. 
Much as being a citizen of the United Kingdom requires paying taxes in pounds
Sterling, if one wants to interact socially on Facebook, one must pay the
personal data tax which they demand. 

In some cases, the provision of personal data is
necessary requirement for the provision of a service, but in many
cases it represents an attempt by the organisation to create a monetizable product from
its users. Many pieces of data fit both goals---for example the addition of 
mobile phone numbers to user accounts may have security benefits for the users,
but it also adds stronger, more persistent ties to offline identities.

\todo{Spiekermann ref}
\cite{Novotny2013DEF,Spiekermann2014}

There is also a spectrum of approaches from outright demands, 
to asking or encouraging users to furnish their data.
Increasingly, \emph{gamification} is used to manipulate
users into self-surveillance, by providing rewards---whether within the system
or through the promise of self-improvement---for activities which require the
sharing of data to function:
\begin{quote}``Literally, within an hour of waking up, I am playing at least 
two games that promise to help me become a more productive
worker and prolific writer. \ldots I want to suggest two things: 1) that gamification is a form of
surveillance; and 2) this surveillance is
pleasurable''~\cite{whitson2013gaming}\end{quote}
Fitness apps, activity monitors, location based social networks require the
user to hand over their location data in return for the promise of increased
fitness, self awareness to the ability to connect with others. 
This user-driven data collection becomes a form of \emph{participatory surveillance}:
\begin{quote}
``Online social networking can also be empowering for the user, as the monitoring and 
registration facilitates new ways of constructing identity, meeting friends and colleagues 
as well as socializing with strangers. This changes the role of the user from passive to 
active, since surveillance in this context offers opportunities to take action, seek 
information and communicate.''~\cite{albrechtslund2008online}
\end{quote} 

In summary, there are multitudinous situations where we are coerced, cajoled or
manipulated into sharing our personal data, and the cost of avoiding such
sharing is increasingly becoming untenable for large sections of the population
of the connected world.

\subsection{Sharing is a loss of control}

Sharing data, by definition, is the entrusting of other parties with
information; this necessarily involves relinquishing control over how it is
subsequently handled and disseminated. While it is possible to attach terms and
conditions to the data at the point of sharing, the sharer must rely on the
compliance of the recipients. Technological means to control information
usage---such as Digital Rights\footnote{Often called Digital \emph{Restrictions}
management by opponents, e.g. \url{www.defectivebydesign.org}} Management
(DRM)---are generally quickly circumvented \cite[p.6]{doctorow2008content}.
Research is being carried out into improving \emph{accountability} (e.g.
\cite{feigenbaum2011towards}), which can help to create a context which limits
breaches of trust, but again, this relies on mechanisms outside the control of
the sharer.

 \todo{eMax: Do we want to mention DRM here as a futile attempt to control how data is used once it's shared?}
 \todo{DMR: I've added a little bit, but feel free to expand! I was looking for
 a ref. on how all drm gets cracked, but couldn't find anything sensible}
 \todo{There is so much more we can say right here about this.}  

There are many issues with sharing data, here we highlight four of them:
\begin{description}
  \item[Sharing is persistent, while situations evolve;] once data is shared,
  there is no technical means to revoke it. However, the context around its sharing and the
  organisations involved are subject to change. 
  A government may decide to share previously confidential data, as in the case of the recent \emph{care.data} fiasco in the UK; a company can be
bought and its assets acquired---the purchase of Moves by Facebook raised issues
around the terms and conditions of data handling companies; and even without
malice accidents can expose vast swathes of personal data, or court proceedings
may force private communications to become public - the Enron emails still
represent the largest publicly available corpus of private emails. Essentially,
once data is shared, the sharer has no control over what happens to it. Once shared, the data becomes persistent. Bruce Schneider commented\footnote{as part of a panel at the ``Don't Spy on Us'' day in London, UK, on 7 June 2014} that we are in the middle of a social change---``the loss of the ephemeral''. He said that ``Google doesn't only know what I think better than my wife does, it knows what I think better than I do because it remembers all I said and did''. As humans we are used to think that what we say will be forgotten sooner or later, and that we can change our minds, which is not the case any more when there is a persistent record of everything we say and do.

\item[Technology improves:] what is safe to share now may not be in the
future. Brad Templeton from the Electronic Freedom Foundation uses the analogy
of ``Time travelling robots from the future'': the information collected now
will be subjected to increasingly sophisticated analysis techniques as time
progresses, so the implications of sharing that information can be far beyond
expectations. For example, in the future, it may be possible to carry out facial
recognition on massive quantities of CCTV footage, and reconstruct the movements
of a large proportion of citizens. This corresponds to the surveillance
robots coming back in time and monitoring us now. In a similar vein: ``Would
you have liked to be gay 40 years ago in a monitored society? Or an enemy of J.
Edgar Hoover with modern tools in his hands?''~\cite{templetonWatched}. Sharing
data today cedes control to the entities of tomorrow, with their greatly
enhanced capabilities.

\item[De-identification is not a magic bullet:] data is often shared subject to
the condition that it will only be shared in an \emph{anonymised} or
\emph{de-identified} form. Whilst sharing de-identified data aids in privacy
preservation, it can also create a false sense of security, since
\emph{re-identification} can take place. This might be purely through a set of
data released, or through the fusion of multiple data sources to provide higher
discriminatory power. As a highly public example, Netflix challenged the public
to create a better recommendation engine, based on a corpus of anonymised viewing histories. Subsequently, it was shown that many records within the database could be identified by comparison with publicly available sources~\cite{narayanan2008Deanon}, let alone access to other, non-public data.
Narayanan and Felten's recent report~\cite{narayanan2014Deidentification} explains in a non-technical manner why
de-identification of data remains problematic. This is also highly dependant on
the data in question: for example, location data is extremely difficult to
anonymise, with four data points being enough for 
re-identification in many cases~\cite{montjoye2013Unique}.

\item[Databases can be joined:] as more databases of personal information
become available, whether publicly or privately, the possibility to match, join,
correlate, and share data increases, and the effects of single points expand well
beyond the environment in which they were created or shared. In short, data are
held in \emph{leaky containers}: ``data move freely between different sectors of
society with the result that information from discrete contexts, e.g.,
 private life, work life and shopping, are being mixed rather than contained 
 separately.''~\cite[p.37--44]{lyon2001surveillance}.
\end{description}

%  \todo{What you are sort of getting at as ``rights'' to data and the lack of
% regulation thereof; there is no notion of data ownership in any legisltative
% form, nor is there a notion of course of maintenance and tansfer of such terms
% of ownership - except in the most select of contextx, people have no legal
% ``right to even their own data''}

%% \subsection{The case for lying and the importance of anonymity}

\subsection{The beneficial uses of ambiguity and lies in social mediation}

Studies in Computer Mediated Communication (CMC) and agent-based modelling of
online communities have studied the use of deception both in real and simulated
settings, revealing a contrast between \emph{pro-social} forms of lying and
deception, and \emph{anti-social} forms~\cite{iniguez2014Deception}.
A wide range of pro-social kinds of deception have
been documented, primarily around coping strategies that people adopt towards
dealing with the demands and complexities of social mediation in increasingly
digitally-connected lives.            

In the first of two strands of work pertaining to such coping strategies, was a
focus on how people often leverage the ``space of ambiguity'' created by
``low-bandwidth'' computer-mediated communications channels, such as SMS (text
messaging) or Instant Messaging, or even voice mobile phone conversations, to
facilitate the use of pro-social deception for the purpose of efficient
mediation.  For instance, an individual might intentionally ignore an incoming
call, or blame poor reception (where there is none) to terminate a call
prematurely, in a situation where a person is stressed with other priorities, or
avoid talking to someone for another reason.  In work by Aoki and Woodruff~\cite{Aoki:2005:MSS:1054972.1054998}, it is argued that such uses of ambiguity
are beneficial to the actors who use them, by providing effective avenues for
exercising control while simultaneously mitigating potentially socially awkward
or damaging situations, with a low social risk.  They also point out that, as digital systems become increasingly
sensor enabled and aware of people's activities in order to provide more
``real-life'' kinds of interpersonal interaction, such spaces begin to collapse,
reducing people's ability to leverage them, thereby potentially reducing
individual autonomy.  The authors argue, thus, that it is essential for
designers to design into them the ability for people to give themselves the
cover of ambiguity to support flexibility in coping strategies for social
mediation.                     

The second pertains to the use of ``butler lies'', a more explicit use of
deception, for the purposes of simplifying social channel mediation; for
example, using the excuse that ``I have a meeting with my boss'', when in fact, no
such meeting exists, in order to terminate a conversation without the other
participant feeling unimportant~\cite{Hancock:2009:BLA:1518701.1518782}.  Among several studies analysing the
use of butler lies in CMC there has been work characterising the volume of such lies,
which, reported in a study by Reynolds et al.
\cite{Reynolds:2013:BLB:2441776.2441862}, amounted to 27.1\% of all
communications mediation messages sent by participants who texted partners and
co-workers. The purpose of such messages was often to mitigate social
awkwardness, reputational damage, and reduce attentional load, and they were used
independent of the nature of the relationship among communicating parties of
their relationship to a person.  Such lies were particularly employed in awkward
situations such as when a speaker wishes to convey a message that ``she is too
busy to interact with the recipient, but not too busy to interact with others''
\cite{Reynolds:2013:BLB:2441776.2441862}.  Moreover, a detailed analysis of the
potential emotional impact of having the lies exposed were overall small and
much less significant than other kinds of lies.                    

The important role of these studies is to demonstrate the beneficial nature of
some forms of deception and information withdrawal towards  coping with social
coordination and communication demand overload.     

% White lies can aid in social network growth: \cite{iniguez2014Deception}

% Most, if not all, social interactions 
% involve both strategic omissions and various kinds of lies and
% non-truths to manage the myriad conflicting social demands placed upon us. 

% Butler Lies: \cite{hancock2009butler}

% Translucence: ``What we say and do with another person depends on who, and how
% many, are watching.'' - \cite{erickson2000Translucence} \todo{This is important
% and should stay\ldots}

% Contrast with Transparent society \cite{brin1999transparent}; power imbalance
% between parties. \todo{Can ditch this if you want!}

\subsection{Multiple Identity}

A natural part of online life is the ability to tailor the persona we present to
different communities and contexts. An individual may want to disclose certain
things to their professional colleagues, while presenting differently to friends
and family or non-mainstream friend groups. 
As an example, prolific content creators on Youtube have multiple online
personae for different kinds of content---``official'' channels, a personal one,
and sometimes channels for characters from their works~\cite{guy2014ConstructedIdentity}.
Bruce Schneier called Facebook ``a collapse of context'' where the personae are
forcibly merged.

Pseudonyms are one way of representing personae. In~\cite{dalton2013Pseudonymity} Dalton describes pseudonymity in social machines as
a one-to-one or a one-to-many relationship---between the human and the
pseudonyms. It is possible for a person to be consistently known by a pseudonym
over several systems, or use many pseudonyms because they are easy to create and
to maintain. However, the many-to-one relation is possible as well, as
illustrated by Anonymous, or by Nicholas Bourbaki in the field of mathematics,
where the same pseudonym is used by a group. The group can evolve in time while
the pseudonym remains. 

\subsection{Why doing it socially is difficult}

Sharing certain personal data is a barrier to anonymity and obfuscation; 
data which is rooted in physical fact provides multiple opportunities for
joining up otherwise separate databases. 
The lie maintenance required to avoid discovery may be trivial (``sorry, I'm hungry,
have to go!'') but may become significantly more complicated as lies extend over
time, and become woven into the social fabric. The ability to compare
multiple accounts of history---especially once the time travelling
robots are involved---means that dissonance within the social
fabric is more obvious than weaknesses in a single thread.




\subsection*{Why verification and provenance are better than sharing}

Sharing is a crude mechanism. Once data has been shared, the originator can no
longer exert control over it, and must rely on the behaviour of the recipient,
which as noted may fail to meet user expectations. As danah boyd
notes: ``Any model of privacy that focuses on the control of information will
fail''. This leads the teenagers that boyd studies to
engage in \emph{social steganography}, manipulating messages so that ``Only 
those who are in the know have the necessary information to look for and interpret the information 
provided.''~\cite{boyd2012Networked}. Strategies like this work when there is a
difference in understanding between the surveilled and the surveiller, and
collapses as soon as the comprehension barrier is removed.

Validation, however is a more subtle tool:
if a user's personal dataset can be made sufficiently questionable as to be useless on its own,
then locus of control shifts to the user choosing to validate parts of the dataset,
which can be performed in a more nuanced,
contextualised manner. If a user is the final arbiter of trust, they can decide
to i) sign parts of their record, so that it is verified public fact; ii)
co-sign it with another entity, so either can  also verify it but not anyone 
else;
iii) verify it through an anonymous channel, so that the entity to whom they
provide verification cannot propagate the claim further. This verification can
be carried out entirely separately from the data store itself, allowing for the
presentation of different datasets as valid  in different contexts, as well as
unorthodox methods such as using the Bitcoin blockchain to notarise datasets, so
that they can be verified in the future without revealing them as true at the
time.

\section{Review of current approaches and tools} 

Privacy tools for end-users of the Web have focused on approaches to allow people to cloak their originating location (IP address) and identity online, as they access web sites that may be instrumented with code from any number of advertising networks and tracker agents.  The most basic of such tools constitute simple browser add-ons that explicitly block the execution and downloading of web beacons, tracking pixels and other tracking agents, through use of a dictionary-approach (of known tracking agents) (e.g., DisconnectMe\footnote{DisconnectMe - \url{https://disconnect.me/}} or AdBlock Plus\footnote{AdBlock Plus - \url{https://adblockplus.org/}}), or using adaptive algorithms to infer cross-site tracking (e.g., Privacy Badger\footnote{Privacy Badger - \url{https://www.eff.org/privacybadger}}).  Similarly, plugins such as HTTPSEverywhere\footnote{HTTPS Everywhere - \url{https://www.eff.org/Https-everywhere}} force the browser to communicate with all web sites using encrypted HTTP, to prevent ISPs and other intermediaries from eavesdropping on traffic between the end user's computer and the end-user host.  The use of HTTPS also thwarts some deep-packet inspection (DPI)~\cite{kumar2006advanced} based tracking methods, such as employed by public WiFi access points such as those at international Starbucks coffee shops, used to determine customers' interests and also to filter and re-rank search results to benefit Starbucks.

In the middle have been a handful of application- or service- specific anti-tracking tools, including TrackMeNot~\cite{howe2009trackmenot}, a plugin which aims to reduce tracking and personalisation done by search-engines by diluting the user's query within a flood of other, random search queries. Similarly, CacheCloak~\cite{Meyerowitz:2009:HSF:1614320.1614358}, intercepts requests for a user's location from various location based services, and returns plausible nearby, but inaccurate, results.  

Peer-to-peer systems such as Bittorrent, Freenet and others pose additional challenges for privacy because a user's client typically directly connects with a large number of other individuals' clients.  This means the client's location (IP address) is typically highly visible within the network, although, in some cases, using approaches like Tor to disguise location (discussed next) work in such settings depending on particular details.  Work on disguising an individual's interests (through queries) within this network, to promote plausible deniability of intent within it include projects such as SwarmScreen~\cite{choffnes2009swarmscreen} and OneSwarm~\cite{Isdal:2010:PPD:1851182.1851198}.  Other approaches, including encrypting all transferred queries and data have been taken by systems such as Mega\footnote{Mega - \url{www.mega.com}}.

Perhaps the most sophisticated current tool developed for obscuring network activity and traffic origin is currently Tor~\cite{dingledine2004tor}, which works at the network-level to hide the origin of packets when communicating with a website or other third party.  Tor works as a peer-to-peer overlay network that routes Web and other network requests through a randomly selected circuit of hosts on the network using the onion routing technique, which makes it intractable to deduce the origin of a particular packet.  In conjunction with end-to-end encryption (such as over HTTPs), Tor has been shown to effectively thwart eavesdropping and DPI methods.

Even with such tools, ISPs, mobile broadband providers and other ``last mile'' internet access providers can collect a significant quantity of low-level data pertaining to the physical place and times that a person accesses sites, the quantity of data exchanged, and the potential destinations.  To thwart this level of tracking, several hardware vendors and operating systems are starting to incorporate the ability to perform identifier randomisation (such as MAC-address and Bluetooth hardware address randomisation\footnote{Such features are expected to be introduced as a standard feature to the consumer market for the first time by Apple in iOS 8 - \url{http://appleinsider.com/articles/14/06/09/mac-address-randomization-joins-apples-heap-of-ios-8-privacy-improvements}}.  Such features are standard for an emerging group of speciality \emph{security hardened} devices, including security-enhanced mobile phones and tablets (such as the Black Phone\footnote{Black Phone - \url{https://www.blackphone.ch}}). 

While these tools are dedicated to obfuscating action and identity for actions in the digital realm, automatic tracking of people and their activities in the physical world has also increased, thanks to improvements in facial recognition, the introduction of digital tokens and keys for granting people access to physical spaces, membership cards ("loyalty programmes"), credit cards, and so forth.  A number of projects and approaches have similarly been introduced to reduce trackability in this space.  In terms of protection against facial recognition algorithms, for example, Harvey et al introduced a special make-up technique based on reverse-engineering the most popular face detection algorithms, called CV Dazzle~\cite{harvey2012cv}. To reduce infrared camera based person tracking Harvey also introduced heat-signature cloaking burqas and hoodies in a line of clothing called \emph{Stealth Wear}\footnote{Stealth Wear - \url{ahprojects.com/projects/stealth-wear}}.

To reduce city-wide tracking using transport cards such as the London Oyster card, NYC MetroCard, as well as  purchase tracking via loyalty card schemes, a common practice that has arisen in several of these cities has been to host social meetups where people regularly gather and swap their valueless cards.  This allows them to blur data mining algorithms by eliminating the assumption that a single card will be owned and used by a single person, but instead by a constantly evolving group \cite{lockton}.

% \todo{Apple patent on cloned identities of the principal user - 
% http://www.forbes.com/sites/andygreenberg/2012/06/20/apple-patents-technique-tha
% t-uses-cloned-doppelgangers-to-protect-your-privacy/}
% 
% \todo{for full disapearance act:  How to Disappear: Erase Your Digital 
% Footprint, Leave False Trails, and Vanish without a Trace by Frank M. Ahearn and 
% Eileen C. Horan. 3 chapters on disinformation and creating false trails}

% \todo{Refs to work in\ldots}
% \begin{itemize}
%   \item Marwick, public domain: \cite{marwick2012Public}
%   \item Reigeluth, data traces: \cite{reigeluth2014Data}
%   \item Beer, algorithms and power: \cite{beer2009Algorithm}
%   \item Goldberg, public/virtual participation: \cite{goldberg2010Rethinking}
%   \item Simon, panopticism \cite{simon2005Panopticism}
% \end{itemize}

% \todo{Max to do some writing}

% \begin{itemize}
%   \item The revolution has started!
%   \item tor, anonymous remailers, burner phones, gotta change up, yo!
%   \item HTTPSEverywhere
%   \item Surveillance and Society: \url{http://library.queensu.ca/ojs/index.php/surveillance-and-society}
%   \item CVDazzle
%   \item Heat-signature cloaking burqas, hoodies
%   \item Bluetooth and MAC randomisation in iOS 8
%   \item Silent Circle, Cryptocat
%   \item DNT in IE10
%   \item Adblock/Adblock Plus, Privacy Badger, Disconnect.Me
%   \item HTTPsEverywhere
%   \item VPNs 
% \end{itemize}

% Open source and trustworthiness
% Theory of obfuscation:
% Types of disinformation \cite{alexander2010Disinformation}:
% \begin{description}
%   \item[redaction] is hiding some or all of the information in a message
%   \item[airbrushing] is changing some of the information. \emph{local crowd
%   blending} means change it to a nearby message likely to be plausible.
%   \emph{global crowd blending} means change it to a message in a dense part of
%   the space.
%   \item[curveball] add extra distracting information, push message into low
%   density space
% \end{description}

% Some existing stuff and the things we can link it to later

% \begin{itemize}
%   \item TrackMeNot generates plausible google searches (Chaff)
%   \item FaceCloak? Encrypts facebook data
%   \item CacheCloak - sends a range of plausible future paths to location based
%   services (Palimpsestification)
%   \item Shopping card loyalty swaps (Account Sharing)
%   \item DuckDuckGo - mixing up user searchers (Account Sharing, no cleverness)
%   \item CVDazzle
% \end{itemize}





\section{A selection of obfuscation strategies}
\label{sec:strategies}

Alexander's taxonomy \cite{alexander2010Disinformation} discusses several types
of disinformation which relate to modifying single messages along schema such
as:
\begin{description}
  \item[redaction], where some or all of the information in a message is
  hidden;
  \item[airbrushing], where some of the information is changed. This can take
  the form of \emph{local crowd blending}, where small alterations are carried
  out so the new message is similar enough to be plausible, or
  \emph{global crowd blending} where messages are heavily altered in order that
  they resemble plausible, but quite different messages.
  \item[curveball], where extra, distracting information is added to messages
  which pushes them into a low density area of message space.
\end{description}
In contrast, due to
the pervasiveness of modern communications, we are concerned with modifying
message \emph{streams}, where a trace of multiple values must be considered.
The social aspect inherent to modern communication tools increases the 
range and frequency of interaction with others, which in turn increases the 
chance of lies being exposed.
However, we can also use these social interaction to our advantage, colluding to
strengthen the obfuscatory practices. %yearbook-obscurity/DEF-Obfuscation.tex
% Additionally, there is the possibility of interaction with others, whether it is
% collusion to strengthen obfuscatory practices, or the addition---purposeful or
% otherwise---of information which exposes the 
%obfuscation.yearbook-obscurity/DEF-Obfuscation.tex

In this section, we present a range of obfuscation strategies, some of which are
speculative, but many of which are drawn from existing examples both inside and
outside the digital sphere.

For each strategy we discuss: \begin{inparaenum}[\itshape i\upshape)]
\item what kind of alteration of baseline data is performed;
\item what is the motivation for doing it; 
\item what are the possible use cases;
\item how some form of computational support aids in the deception;
\item some of the systems (if any) which do this currently.
\end{inparaenum}

It is problematic to consider the obfuscatory tactics here without a sense of
the scenario in which they are to be deployed. Our scenario in this chapter is:
\begin{quote}
The user wishes to make use of services which expect location information; 
the location information provided is shared publicly and is almost
certainly stored indefinitely. At times, the user may want to draw on location
based information---such as restaurant recommendations or directions---and there
may be times when they wish to verify that they were at a particular location.
\end{quote}
The service is hence \emph{semi-trusted}: there are some benefits which the user
wishes to accrue, but there are aspects of the service which makes the user
unwilling to entrust their complete location history to it. We have chosen location
as a clearly understandable facet of personal data, and one which can be easily
used to re-identify individuals from anonymised
datasets\cite{montjoye2013Unique}.

\fig{Mediation}{Models of interaction with semi-trusted services. a) Direct
transmission of information; b) computationally mediated transmission, where a
personal data store is enlisted to aid in obfuscatory processes.}

The standard model of interaction (Figure \ref{fig:Mediation}a) involves the
user submitting their data directly to the service; for our obfuscatory
techniques, we would like to enlist computational support (Figure
\ref{fig:Mediation}b). This typified, but not limited to mediation from a PDS
which acts on behalf of the user to modify the data which they provide.
In Figures \ref{fig:SinglePlayerObfuscation} and
\ref{fig:MultiPlayerObfuscation}, we plot a fictitious one-dimensional
``location" measurement against time in order to give a sense of how
obfuscations unfold across time in multiple locations\footnote{While a two
dimensional, map-based representation would be more immediate, it is difficult
to clearly show temporal aspects.}. We show the individual's true `location' as
a continuous line, along with the measurements made by their device; we then
overlay the points which would be submitted on their behalf after obfuscation.

\subsection{Strategies for the lone obfuscator}

\fig[1.05]{SinglePlayerObfuscation}{Obfuscation strategies for the lone agent}

Figure \ref{fig:SinglePlayerObfuscation} lists a collection of possible
obfuscation strategies. In all cases, a fictitious one-dimensional ``location"
measurement is plotted against time, to give a sense of how an
individual's position in space changes. Figure
\ref{fig:SinglePlayerObfuscation}a is the true baseline, with a 
curve indicating the continuous true position, and the dots
representing reports of this position to the location-aware service. 
% For each strategy we discuss: \begin{inparaenum}[\itshape i\upshape)]
% \item what kind of alteration of baseline data is performed;
% \item what is the motivation for doing it; 
% \item what are the possible use cases;
% \item how some form of computational support aids in the deception;
% \item how the strategy can be applied to other data
% \item some of the systems which do this currently.
% \end{inparaenum}

\subsubsection{Chaff}

World War II fighter planes would emit clouds of radar reflective
sheets---\emph{chaff}---which created multiple traces the screens of radar
operators, and hence disguised the true position of the aircraft. In a similar
manner, we can add in multiple location data points alongside the real ones. This
is the one of the few methods where the complete, accurate data stream is stored.
Hence the user can still access any benefits which rely on accurate information. However,
adding a multitude of randomised points to a service which expects a single
contiguous trace is both easily detectable, and may break functionality---a
run tracking application would be likely to give unreliable distance estimations
in the presence of chaff.

\subsubsection{Noise injection}

The most computationally simple form of obfuscation is the addition of noise to
the reports which are sent to the semi-trusted party. Here, the points which are
submitted deviate from the true values in a random manner. This allows the user
to conceal their exact location, while giving a broad indication of where they
are. Depending on the level of noise, this can allow the use of location
based services without revealing much about actual behaviour. For example, it
might reveal your location on the high street so you can arrange to meet
friends, without revealing which shops you were visiting. This is compatible
with services which expect coherent location data, and may be indistinguishable
from the inaccuracies of the location sensors. One downside is that the ``true''
location traces are not present in the record of the service.
% \todo{examples?}
For example, TripAdvisor can still provide a good enough list of recommended 
attractions around the given ``noisy'' location, however a navigation 
application will not be able to provide reliable directions.

\subsubsection{Coarsened Granularity (or Quantisation)}

Rather than adding noise to the data being sent, it can instead be quantised to
a coarser granularity, akin to blurring, or zooming out on a map. Again, this is
a technique which may help to derive useful information from the service,
without revealing more than is necessary: using a service to find friends in the
same city should only require city level information to be shared. An example of
this can be found in Android's permission system, which has separate controls
for \verb|ACCESS_COARSE_LOCATION| versus \verb|ACCESS_FINE_LOCATION|; similarly,
posted letters may be signed with a city rather than a street address.

\subsubsection{Systematic Deviation}

In some cases, it may be possible to introduce systematic deviations into the
digital record. In order to do this, the user needs to be able to define which
points to alter, and what to replace them with. One possibility would be
thematic replacement---``hide the times I went to the pub by saying I
was at a cafe''. Another would be to disguise the user's home and work
locations---places where they are less likely to require location based
searches, but which make it very easy to re-identify them from anonymised data.
It is likely that this will require some form of computational support to i) identify targets for replacement as they occur and ii) find suitable replacements. Using this technique, some, but not all of the true data is stored; however derived information---such as beverage preferences in the
example above---can be wildly and purposefully distorted. The nature and fact of
the distortions may be hard to uncover, as no simultaneous traces or
strange movement patterns are produced. Depending on the domain, subtle
alterations may have large effects.
% \todo{examples?}

\subsubsection{Pretend to be me}

With increasing computational support, it becomes possible to create a model of
the user which outputs plausible ``normal'' data. This can then be used to
replace periods of abnormal behaviour, or even replace normal behaviour with
statistically similar but untrue data. An early example is when neighbours (or
automatic switches) are employed to turn lights on and off in a home which has
been vacated for the holiday, disguising the true anomalous data of a dark,
empty house with the appearance of normal occupation. Similarly, one might avoid
making Facebook posts which indicate an absence, to avoid burglary. This kind of
deception can be difficult to achieve; however computational systems are
emerging which can aid users, for example Beyer's digital alibi system
\cite{beyer2014Alibi}.

\subsubsection{Coherent Deviation}
As the converse of simulating normality, the user may wish to pretend to be
somewhere where they are not. 
% \todo{more motivation for Alibot!} 
This is similar
to creating systematic deviations, but on a grander scale; the user would like
to create a narrative for the deviation, and then have suitable data points
constructed. For example, the use might pretend to be on holiday, or at a
conference, and would like location traces which match that narrative to be
created, such as going to the convention centre in the day, and returning to a
hotel at night. This requires a computational model of user behaviour which can
be applied to new locations---a non trivial task. However there is the potential
to create obfuscated data which is difficult to distinguish from standard
behaviour. Services like Please Don't Stalk 
Me\footnote{\url{http://pleasedontstalkme.com/}} enable geotagging tweets 
with fake location information, and the Chrome browser supports faking location 
information in the browser, through the Chrome Developer Tools emulator. These 
tools can be used for some of the previous strategies, for noise injection by 
using random locations, or for systematic deviation, when only the location of 
certain tweets is modified.

\subsubsection{Palimpsestification}
Taking the idea of coherent deviations a stage further, and combining with the
idea of \emph{chaff}, the user could create multiple overlapping traces; each
trace would be locally coherent and plausible, but someone inspecting the data would have no way to know which is the real one.
This is similar to the strategy of 
CacheCloak~\cite{Meyerowitz:2009:HSF:1614320.1614358}, which
continuously generates sheaves of probable future behaviour and searches
location based services relevant to each  path. The computational support
required is similar to the coherent deviation example---to be able to run a
model of the user's behaviour in novel locations---although more coordination
might be required between the stories. The trade-off is that while the true
location data can be entered along with the generated points, the deception is
obvious, and location based services may become upset at the multiple paths.

\subsection{Collaborative Obfuscation}
\fig{MultiPlayerObfuscation}{Multiplayer obfuscaction strategies: i) artificial
co-location; ii) supporting information; iii) account sharing}

Including others in the obfuscation challenge opens up a range of new 
strategies, where collusion can aid in the creation of otherwise unachievable
data streams, or increase the veracity of artificially created data. Generally
the possibilities in computational systems are analogous to pre-computational
possibilities; computational support tends to be in the form of coordination to
find collaborators or and check coherence of data points. The ideas outlined
here are more speculative, as few computational systems of this type exist.
There are aspects which make these strategies harder to pull off as coherence is
required across multiple different accounts; however the counterpoint is
that if successful, the obfuscation is better supported and harder to detect.

\subsubsection{Artificial co-location}
One way to obtain a realistic but untrue location trace is to re-present the
trace of a collaborator. This can look like relatively natural
behaviour; two people meeting up to carry out joint activities or socialisation.
Computational support here can involve finding accomplices to ``co-locate''
with---people who are willing to share their location, and are behaving in ways
which match the desired story---as well as the technical details of
transferring location devices between accounts.

\subsubsection{Supporting Evidence}

% \todo{Maybe merge into preceding co-location section}
Co-located people often share the fact of their co-location, explicitly or
implicitly, whether in group photos---``Here's me and X on top of the Scott
monument''; broadcast messages---``Just been hanging out with X at the coffee
shop''; or shared plans---``Going to the cinema with X tonight - anyone want to
join in?''. Enlisting collaborators to make these kinds of posts can help to add depth to a
constructed trace, weaving it more tightly into the social fabric.

\subsubsection{Account Sharing}

In a similar manner to the swapping of loyalty cards discussed in
\cite{brunton2011vernacular}, users of services can share accounts. This
results in an account or set of accounts with more or less plausible activity,
yet allows the users to remain unidentified. 
Much as the loyalty card swapper confounded efforts to inspect individual
buying habits, or the ``Anonymous'' movement aggregates the activities of a
multitude of participants behind a stylised mask, services such as DuckDuckGo
aggregate many people's search results, ensuring that the search providers
cannot build up any identifiable user histories. 

Computational intelligence can be enlisted to support many different ways of assigning people to accounts,
 such as:
\begin{description}
  \item[\emph{Many to one}] schemes have a single account controlled by
  multiple people. This can result in a completely incoherent manner;
  DuckDuckGo's aggregated search makes no attempt to imitate
  individual behaviour. 
  Alternatively, sharing can be tied into a coherent shared identity,
   where multiple people contribute to a single shared persona\cite{dalton2013Pseudonymity}. 
   Here there is a challenge to maintain consistency: when multiple people
   control a call centre's chat avatar, they must ensure that the relevant
   information and state is shared [\emph{ibid.}]. When multiple users control a
   single game character, the game world enforces consistency, and the community
   must produce coherent action streams in order to progress.
%    \todo{cite Twitch!}.
   \item[\emph{Randomised}] schemes allocate accounts to people without a
   guiding principle; when loyalty cards are mailed between anonymous
   participants, there is an explicit desire to produce implausible data in
   order to confound analysis. Online accounts can be similarly shared, leading
   to traces which are unlikely to have been produced by a single individual.
   In our locative service example, this allows users to access benefits which
   do not rely on individual history, while preserving some level of privacy.
   Computational support involves finding accounts to share, and ensuring that
   each account is only accessed by a single person at any given time.
   \item[\emph{Structured}] schemes allow for accounts to be used as appropriate
   according to some criterion. If a location service offers history based
   benefits (e.g. loyalty rewards or reputation) then it could be beneficial to
   borrow a local user's account when going on holiday---couchsurfing but with
   login credentials instead of flats. Infrastructure would be required to
   discover appropriate accounts, and mediate access. 
%    \todo{More explanation? Link to powerlevelling in WoW?}
\end{description}

\section{Operationalisation - managing deception and its side effects}

\subsection{Going beyond location}

In Section \ref{sec:strategies}, we discussed obfuscatory possibilities with
respect to a location based service; however, this is a single application area,
and the need for regaining informational autonomy is felt across spectrum of
data types and services, hence we must discuss how these techniques generalise.

Location data is generally collected by a device which the user owns. In many
cases, this is a smartphone, which uses a combination of GPS, cell tower
triangulation and WiFi access point locations to determine a user's position in
space. The user then has some level of choice about who to share the data with
and how. This is not always the case, however: cell tower records can identify
user's locations---and individuals can be picked out from very sparse
histories\cite{montjoye2013Unique}. Smart phone manufacturers collected 
location data from their users without their knowledge---which regardless of the 
possible good intentions or technical reasons\footnote{Apple's response:\\\url{http://www.apple.com/pr/library/2011/04/27Apple-Q-A-on-Location-Data.html 
}} behind the logging, led to a public outcry when it was revealed in April 
2011\footnote{\url{O'Reilly Radar: http://radar.oreilly.com/2011/04/apple-location-tracking.html}
% \\ArsTechnica: \url{ 
% http://arstechnica.com/apple/2011/04/how-apple-tracks-your-location-without-your 
% -consent-and-why-it-matters/}, \\Wired: 
% \url{http://www.wired.com/2011/04/apple-iphone-tracking/}, \\WSJ: \url{ 
% http://online.wsj.com/news/articles/SB10001424052748703983704576277101723453610} 
}

Obfuscating data (location or otherwise) can be done only if the data is 
known to be collected in the first place. Some knowledge about the data 
collection process---what data is captured, when---helps improve the obfuscation. 

Some data, even when known to be collected, cannot be obfuscated. Examples 
include all official data like census data, police reports, tax returns, health 
records, but also private organizations with a regulated status like banks and 
utility providers. Obfuscating this types of data in any way can be considered 
a criminal offence. 
For example ``obfuscating'' one's income source by adding chaff or noise can be 
considered tax evasion, theft, money laundering, etc.
% ; obfuscating energy consumption data can lead to suspicion of growing marijuana 
% (in some places), or running illegal servers (in other places). 
% \todo{do we want to go further?} Even weirder, obfuscating family membership :) 
% false number of children / parents / supported or supporting family members. 

For data that can be obfuscated without legal consequences, and for 
which we are aware of the data collection, we can use the same 
strategies as described in Section \ref{sec:strategies}, with adaptations 
depending on the data type. 

For a photo sharing application or microblogging tool, adding \emph{chaff} 
could mean posting more content, but fabricated. 
It can be realised by posting public domain photos on Instagram from a randomly 
chosen topic, or flooding Twitter with randomly generated text, older posts, or 
fragments from existing texts like web pages or books. For browsing history in 
an online shop, we can use bots or browser extensions, or tools like Selenium\footnote{\url{http://docs.seleniumhq.org/}}
to simulate browsing of various items by randomly requesting 
pages beside the page we are interested in. While it does 
not hide the fact that a person visited the shop, it can obfuscate their interest. 
This can be useful when dealing with Amazon, or another large scale marketplace, 
but it can be less useful when the online shop is catering for a small niche 
market. 
The goal of chaff is not to be believable on deeper inspection, or even 
consistent, so the algorithms for choosing what to post and where to browse 
need not be too complex. 

Similar to using prepaid disposable [burner] phones to avoid surveillance, 
ghost email can be used to obfuscate online activity, as it prevents the email 
address from being used as a global identifier for the person. For websites which 
require registration, we can use disposable email addresses, which would 
forward the email received to a real email address. With 
the help of disposable email provider services\footnote{A comparison between 
some disposable email providers is available on LifeHacker 
\url{http://lifehacker.com/5306452/how-do-you-keep-your-email-address-private}}, 
we can use a different email address for each registration, while still 
receiving the email confirmations in our real inbox. These email addresses can 
of course still be linked back to the true user, if the service is compromised 
in some way. For truly disconnected disposable email addresses, services like 
GuerillaMail, 10 minute mail, etc. provide temporary email addresses with a 
short life span. 

Applying \emph{noise injection} or \emph{coarsened granularity} strategies can 
limit the benefit of some services, and in some cases defeat the purpose of 
using them altogether - like injecting noise in the images posted on Instagram.
% In an online shop, can make use of features like ``similar items'' to get 
% information about the actual thing we want to know, but it is tricky and would 
% not work everywhere. 

These strategies can however be used successfully with optional data that 
is required by some services but do not impact on the service provided. 
Randomly changing your date of birth or hometown, or even your name on Facebook 
or Twitter will still allow you to use the service to connect with friends or 
post updates respectively. Changing (injecting noise into) the date of birth 
can be used as the real date of birth approaches, to avoid the 
congratulatory messages, and reverted back once the date has passed. A 
different type of changing granularity is withholding the year from the 
date of birth, thus allowing the actual date be know, without revealing the 
age. Any functionality of a service that is regarded of low importance can be 
used as a place where noise can be injected, or where the granularity can be 
coarsened. This of course leaves part of the information unobfuscated. 

Changing the declared date of birth can be also \emph{systematic}, a user can 
choose an untrue date which to reuse in \emph{all} services that require it. 
Using the same completely fabricated persona in all online communication is 
supported by services like Fake Name 
Generator\footnote{\url{http://www.fakenamegenerator.com/}} which will not only 
create geographically localized names, but also provide valid additional 
information like address, credit card, email address, phone number, mother 
maiden's name, username and password, employment information, weight and height 
and blood type, and even a favourite colour. Using the persona systematically 
and consistently enables a complete separation between the real person and their 
presence online. It is difficult, of not impossible to use these personae when 
dealing with official (regulated) services, as mentioned above.
% example of thsi is the way Peter uses google services under a diff 
% name Ambrose P.

Several personae can be created and used to simulate collaborative obfuscation 
strategies, where the fake characters mimick the existence of supporting 
evidence for the lie. The practice known as \emph{astroturfing}, is used to mask the origin of online 
posts supporting (or contesting) a topic, organization or political message, 
and make them look like a grassroots movement. It can however be used for the obfuscation of individuals' data, by corroborating claims.

% \todo{add more about collaborative strategies? there is already a lot in the 
% prev section}

% \todo{if we use different persona to represent us online, we need dthe 
% collaboration of friends to be able to use social networking services like fb}

\subsection{Personal Data Stores - allies on the intimacy battleground}

Personal data stores (PDS) represent a partial solution to issue of
presentation: having trusted, user controlled repositories for data enables a
more user-centric approach to management of capta---those data which we choose
to take and preserve. Bridges can then be built between personal data stores and
the rest of the world in order to support the connected, networked interactions
which users now expect. If these bridges simply share the data, even in a
controlled manner, nothing has been gained; hence the bridges become conduits
for manipulating truth and constructing falsehoods. As personal data stores
accumulate more real-time contextual data about the individual, as well as about
the individual's social connections, PDSes can provide support for the often
stressful and mentally burdensome task of lie maintenance, for example: i)
identifying when a person's real activities or whereabouts contradict a lie, and
might be discovered; ii) identifying indirect social channels that could expose
a lie (e.g. through friends of friends); iii) suggesting appropriate lies to use
which are least likely to be detected; iv) suggesting individuals to lie to to
support lie maintenance (e.g. friends of the person being lied to).


\subsection{Verification and provenance mechanisms}

\fig{Verification}{Example verification scenario. The user (Ally) provides a set
of real data, plus \emph{chaff} to a location aware service. A third party
(Brett) then requests verification of some of the points, which Ally
provides. Brett then wishes to share the data with Charlie, which requires Brett
to verify to Charlie that the data are correct.}

In the introduction we suggested that verification is a more nuanced mechanism
than control over sharing, since sharing is impossible to control
technologically. One of the effects of the obfuscation strategies discussed
previously is that it becomes impossible to know which parts of the user's
data-stream are grounded in reality, and represent ``true'' values. This means
that if someone wishes to engage with the data and have an expectation of
accuracy, they need to ascertain which parts of the record are correct. This
shifts the locus of control from the process of sharing to the process of
verification---the user can make claims about subsets of the data points
currently attributed to them.

Let us consider a scenario where Ally has some personal data, which Brett would
like to make use of. Brett also wants to sell Ally's data to Charlie.

There are a range of statements which Ally can make, 
including:
``this subset of data points is mine'',
``these points are within 50m of my true location'',
``these points are representative of my general behaviour'' and so on. 
The choice of which point to claim can be negotiated
in the context of the question being asked, and Ally can determine what is
and is not acceptable.

If the external agency wishes to disseminate the users data, it becomes an issue
of propagating the trust which the user has given them---essentially, Brett must
say to Charlie: ``Ally has verified these points to me, and now I am
verifying them to you''. The manner in which the initial verification was carried out now becomes
critical:
\begin{itemize}
  \item if an email or similar communication is used, Ally simply declares
  ``these points are mine'', then the secondary verification is only as strong as
  trust in the communication chain---Brett must convince Charlie that the email
  or message is genuine and emails are easy to fake;
  \item Ally could use a technique which would give Brett no future tangible
  proof of the verification---for example, a single use URL which
  lists IDs for the correct points. Brett would have no evidence with which to
  convince Charlie that Ally had verified the points, other than reputation
  alone.
  \item Ally can cryptographically sign the claim using public key cryptography.
  The claim is then essentially public knowledge, and anyone can check Ally's verification.
  \item Ally can sign the claim after Brett has; this means that Brett cannot
  hide the fact that they were the recipient of the claim, so it is impossible
  to propagate the claim anonymously.
\end{itemize}

All of these techniques relate to making the public record so unreliable that
anyone who wants to use any of the data will need to separately establish a
chain of provenance for certain parts of it. A related goal would be to make it
illegal, or at least unacceptable, to use personal data without having a valid
provenance chain for it. Essentially, in order to use anyone's data, Charlie
would have to explain how they came to have it, and be able to prove that Ally
had shared the data originally.


\subsubsection{Notarisation}
\fig{Notarization}{Notarization of personal data. a) Data points and times are
hashed, and the values sent to a notary service, which provides a URL to verify
that i) the given data was supplied and ii) when it was supplied. Hashes are
used so that the data is not publicly shared. b) If the hash of the previous
submission is included, then sequences of consecutive points can be verified.}

The verification examples above rely on Brett trusting Ally about which
data points are correct. There may be times---e.g. when creating alibis---when
Ally needs to have a stronger form of proof.

In this case, third party digital notarization services can be
employed\footnote{e.g.
\url{http://virtual-notary.org/}, a free service hosted at Cornell University}.
These services take in some document or datum, and provide a certificate which
can be used to verify that that piece of data was provided at a certain time. 
For example, if someone wants to make a prediction
for the outcome of a football match, they could notarize that before the match,
and then subsequently prove that they had made the prediction beforehand. It is
generally not possible to prove that they only made a single prediction, so this technique is most suitable when the range of possible things to notarize
is so large as to make notarizing the entire space infeasible.

With regard to personal data, we can notarize our true data stream as we produce
it. This means that we can prove that we had considered those points at the
time, and if we say we were in a particular place, there is a high chance we
were---however, it does not work in the complementary situation as producing a
notarized point does not prove that we were not anywhere else.


Notarization does not necessitate revealing the data itself. For instance,
when submitting a location, a representation of the time and place could be
hashed, and this hash notarized (Figure \ref{fig:Notarization}a). Additionally,
points can be notarized in sequence, so that we can demonstrate contiguous sub-sequences 
of points as having been provided previously; by
hashing the current location with the previous location, we can link the points
together, to build up confidence in the notarized results (Figure
\ref{fig:Notarization}b).

\subsection{Effects and ethics of obfuscation}
% \todo{How do services react when we stuff them full of noise?}
% \todo{Is it OK to do this stuff?}
Brunton and Nissenbaum discuss at length the ethics of obfuscation in
\cite{brunton2011vernacular}, and address issues like wastefulness, dishonesty,
free-riding, pollution, and possible system damage. One answer is ``that
obfuscation has no ethical or political valence of its own, only to the ends
that it serves'' [ibid.]. However, with the sharing of personal data, there are
multiple overlapping contexts in which the obfuscation takes place, which should
be dissected.

On a \emph{personal} level, obfuscation changes the picture of us that a service
has. For different services, different levels and types of distortion may be
bearable or desirable: while adding noise to a location signal may still allow
sensible location based recommendations, pretending to like random products
might make a recommender service unusable. 
 One of the reasons so many obfuscation strategies were presented is
that each strategy preserves different qualities, and the choice of what and how
to obfuscate must be made in context, as there is an unavoidable tradeoff
between privacy preservation and maintaining the level of personalisation that
makes a service worthwhile.

On a \emph{social} level, being caught in a lie can be unpleasant; as
Bok says:
\begin{quote}
``In practice, however, lying to enemies has enormous drawbacks...  lies to
enemies carry very special dangers of backfiring. All too often, the lie
directed at adversaries is a lie to friends as well; and when it is discovered,
as some always are, the costs are high.'' \cite[p. 141]{bok1978lying} 
\end{quote}
While the reputational damage from lying can be large, some steps can be taken
to mitigate this. Obfuscation techniques can be chosen which are
clearly non-true to human observers, or to one's trusted circle of friends,
echoing boyd's \emph{social steganography}\cite{boyd2012Networked}. The
alterations will then have the desired effect of confounding automated
surveillance, without giving one's friends the sense of being deceived.

Additionally, systematic data manipulation can result in presenting an online
persona which diverges from one's own. Recently, Mat Honan tried the experiment
of ``liking'' everything on Facebook \cite{honan2014Facebook}. After two days of
adding this \emph{chaff} to his activities, his public persona demonstrated
radical far-right and far-left viewpoints, to the extent that his friends became
concerned.

As well as presenting an untrue picture of oneself, systematically adding noise
pushes a cognitive burden onto one's associates. Mat's friends suddenly found
themselves bombarded by the stream of chaff that he was spewing out, to the
point where his junk data edged out all of their normal social interaction.

On a \emph{societal} level, while obfuscation overlaps both `pro-social' lying
and `butler lies', it must also be recognised that systematic mistruths can weaken the social fabric,
disrupting trust with friends and coleagues, as well as those
carrying out surveillance. This adds friction to online interaction, as one must
put the effort into constantly questioning what data is true and false.
On a broader scale, the social good which comes of having access to increasingly
detailed personal data can be compromised if significant proportions of the data
are untrue. 


In terms of \emph{accountability}, there is the danger of being unable to
support your real behaviour: even using a scheme where the real data is
submitted and notarized, any defense which relies on a stream of information
which has been systematically manipulated is likely to be problematic. As online
systems are increasingly being used to account for who you are and what you do,
making your online persona questionable could have far reaching consequences. A
complementary risk is that the constructed data might be accidentally
incriminating: a spuriously generated location point might place a user at a
crime scene, or a participant in an account sharing program could use a
borrowed account while engaging in criminal activity.
There is also a danger that the same tools that preserve personal
privacy are applicable to covering up illegal or anti-social behaviour (e.g.
cyber-bullying). 

Finally, at a \emph{systems} level, the context is dynamic, and services will
respond to obfuscatory practices. 
Services may
shut down or transition to a payment based model in response to the declining
value of increasingly noisy data.
Future services will be better at spotting 
obfuscated data, and may take steps to prevent it, analysing the coherence and
plausibility of incoming data. Obfuscation systems will have to evolve along
with the context in which they are used, creating an arms race between
obfuscators and service providers.

These issues suggest firstly that obfuscation is a form of \emph{free
riding}\cite{brunton2011vernacular}: if everyone did it the system would grind
to a halt, but if a few people do it, they can take advantage of everyone else's
truthfulness to preserve their own privacy. However, if the majority of the
population engaged in obfuscation, this would be a clear signal that the balance
of privacy was unacceptable to the general population, becoming a voice of
protest.

Secondly, it is clear that more understanding is needed around what kinds of
obfuscation to apply when, and how to create tools to enable these practices in
the context of connected, visible, online social behaviour.
% \begin{itemize}
%   \item Viability - how do services react when we fill them full of noise?
%   Plausible versions of these techniques
%   \item Ethics - is this OK?
%   \item Obfuscation evolves in lockstep with systems to see through it; future
%   people will be better at spotting constructed points.
%   \item In the short term services will start to become more suspicious about
%   the data that goes into them; start rejecting points which represent causality
%   violations.
% \end{itemize}

\section{Conclusion}

In this chapter, we have presented a particular approach to shifting the
boundary between privacy and use of data. We have taken a purposefully
adversarial approach to the protection of personal data as an indication of
potential steps which can be taken by individuals at a grass-roots level. 
These techniques are not the final word in personal data sharing, but sketch out
a particular position in the journey towards a balanced societal attitude
towards surveillance.
In time we hope that stronger legal and social protections of personal data will be
introduced, so that society can enjoy the benefits of data sharing while being
respectful of individual's right to
privacy\cite{rooney2012OpenData,rauhofer2012FutureProofing}. 



%%%%%%%%%%% The bibliography starts:
%\begin{thebibliography}{99}
%\bibitem{r1}
%\end{thebibliography}

\bibliographystyle{abbrv}
\bibliography{palimpsests}


\end{document}
