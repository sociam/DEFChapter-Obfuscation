
% PLEASE USE THIS FILE AS A TEMPLATE FOR THE PUBLICATION 
% Check file IOS-Book-Article.tex
%

\documentclass{IOS-Book-Article}     %[seceqn,secfloat,secthm]
%\usepackage{mathptmx}
%\usepackage[T1]{fontenc}
%\usepackage{times}%
%
%%%%%%%%%%% Put your definitions here
\usepackage{paralist}
\usepackage{graphicx}
\usepackage{color}
\usepackage{url}


%Make figures easier
\newcommand{\fig}[3][0.9]{
\begin{figure}[tp]
\begin{center}
\includegraphics[width=#1\textwidth]{figures/#2}
\caption{#3}
\label{fig:#2}
\end{center}
\end{figure}
}

\newcommand{\tbox}[3][red]{{
\color{#1}\noindent{
   \fbox{\scriptsize{ {\bf #2} \textsl{#3}}}
   \vspace{2pt}
}
}}


\definecolor{darkgreen}{rgb}{0,0.4,0}
\newcommand{\todo}[1]{\tbox{TODO:}{#1}}

%%%%%%%%%%% End of definitions
\begin{document}
\begin{frontmatter}          % The preamble begins here.
%
%\pretitle{}
\title{Social Palimpsests - clouding the lens of the personal panopticon}

% Two or more authors:
\author[A]{\fnms{Dave} \snm{Murray-Rust}},
\author[B]{\fnms{Max} \snm{Van Kleek}},
\author[B]{\fnms{Laura} \snm{Dragan}},
\author[B]{\fnms{Nigel} \snm{Shadbolt}}

\runningauthor{Murray-Rust et. al.}
\address[A]{Centre for Intelligent Systems and Applications,
Department of Informatics, University of Edinburgh}
\address[B]{Web and Internet Science Research Group, Electronics and Computer Science, University of Southampton}

\begin{abstract}
The use of personal data has incredible potential to benefit both society and
individuals, through increased understanding of behaviour, communication and
support for emerging forms of socialisation and connectedness. However, there
are risks associated with disclosing personal information, and present systems
show a systematic asymmetry between the subjects of the data and those who
control and manage the way that data is propagated and used. This leads to a
tension between a desire to engage with online society and enjoy its benefits on
one hand, and a distrust of those with whom the data is shared on the other.
In this chapter, we
explore a set of obfuscation techniques which may help to redress the balance of
power when sharing personal data, and return agency and choice to users of
online services.
\end{abstract}

\begin{keyword}
Obfuscation; Data politics; Personal Data Stores; Social Machines; 
\end{keyword}

\end{frontmatter}

%%%%%%%%%%% The article body starts:

\section*{Introduction}

%% maxxie wrote some stuff here

In 1890, Warren and Brandeis called for consideration of new laws to better
secure the ``the right to be let alone'', as ``the private lives, habits, acts
and relations of individuals'' were perceived to be under threat by the emerging
technologies of their time - including ``instant'' photography, and early
electro-mechanical recording devices~\cite{warren1890right}.  Over a century and
a quarter later, technology still continues to threaten individuals' right to be
left alone, now at unprecedented scale and fidelity. In terms of technological
sophistication, the massive computational capacity hosted in various data
warehouses are being used to track individual's daily activities, communications
and locations.  Such profiling comprises a race to build the most complete
picture(s) of each person's life, so as to be able to apply this in-depth
knowledge  to precisely target at them, advertising and other behavioural
manipulations -- often in ways too subtle to be perceived (e.g.
\cite{Kramer17062014}).    

%%  only been met by small steps protecting them from such unwanted tracking and surveillance.  This tracking has, for the large part, become so pervasive that, despite the best efforts of various parties to try to keep such tracking mechanisms covert, it has nontheless risen to be a primary issue in the public eye, meaning that the old ``I have nothing to hide; therefore I should not care'' argument has swiftly been dissolvd.  Previously theoretical arguments of what \emph{might} happen if one is pervasively tracked have been replaced by the tangible reality of the hundreds, 

Such behavioural manipulation driven by omnipresent observation brings to mind
Bentham's thought experiment of the Panopticon, developed by Foucault,
describing a prison in which inmates' lives are constantly surveilled as a means
of discipline and exertion of control.  In the world of technologically-driven
data collection which we inhabit, this has effects both on people's behaviour as
they internalise the fact that they are surveilled, and on the way in which they
are treated, and socially sorted by the gears of our algorithmic
society~\cite{simon2005Panopticism}.        

Meanwhile, the rise of tracking has not been met with a corresponding increase
in either public awareness of how and when they are being observed, nor
practical ways with which individuals can protect themselves from unwanted profiling and
targeting.  As a result, a great asymmetry has arisen between the information
brokers with vast resources that track, and those end-user citizens being
tracked with little or no choice.  As if this asymmetry was not problematic
enough, the few privacy-enhancing tools that have proven effective against
surveillance have been often portrayed by mainstream news and media outlets as
``tools for terrorst and paedophiles'' further discouraging their use and
adoption in the mainstream (e.g.~\cite{bbctor,fbicommunities}).          
% 
% Such asymmetry means that privacy protection can no longer be merely a concern 
% for a minority with peculiar sensitivities towards being seen or
% exposed, but instead, as necessary preserving
% individual autonomy and well-being, in an economy where personal data has become
% the most precious digital commodity. 

% has not been met with a corresponding increase in counter-tracking and counter-surveillance tools

% The ``I have nothing to hide; therefore I should not care'' argument \cite{} used often in defense of ignoring
% privacy in the early days of targeted advertising and the Web still re-surfaces
% today. But as theoretical arguments of what \emph{may} become have been replaced
% by tangible examples of what has actually has, this, among other arguments have
% become thoroughly dismantled.  The sophisticated methods being used to track and
% pinpoint details of individuals has not been met with a corresponding increase
% in counter-tracking and counter-surveillance tools; as a result, people are
% being subject to an unprecedented barrage of targeted attempts to behaviourally
% manipulate them - often in ways too subtle for them to perceive (e.g.
% \cite{facebookstudy}).           

% The use and development of tools for protecting people's privacy online have 
% been framed as methods for empowering and enabling criminals, miscreants 
% and even terrorists to more easily conduct their misdeeds against humanity
% without consequence.  The ``I have nothing to hide; therefore I should not care'' argument \cite{} used often in defense of ignoring
% privacy in the early days of targeted advertising and the Web still re-surfaces
% today. But as theoretical arguments of what \emph{may} become have been replaced
% by tangible examples of what has actually has, this, among other arguments have
% become thoroughly dismantled.  The sophisticated methods being used to track and
% pinpoint details of individuals has not been met with a corresponding increase
% in counter-tracking and counter-surveillance tools; as a result, people are
% being subject to an unprecedented barrage of targeted attempts to behaviourally
% manipulate them - often in ways too subtle for them to perceive (e.g.
% \cite{facebookstudy}).           


In this chapter, we are interested in practices which can cloud the lenses of the observers, to
return to people the choice of how they are seen, and to help regain some
control of
the manner in which our lives are surveilled, both socially and otherwise. 
We discuss and analyse methods of privacy protection that
advance beyond the current state of anonymisation tools that obscure the tracks
of individuals, towards those that employ methods borrowed from
information warfare~\cite{gursestranslating,lyon2007surveillance}, 
in order to allow individuals to regain autonomy
from unsolicited tracking and behavioural control.  We first discuss the rise of 
personal data economy, followed by a survey of lying and falsification in context-aware 
systems, current anonymisation and privacy tools. This is followed by an overview of
strategies for obfuscation, some of which are currently implemented in either
mainstream tools or proof of concept studies,  and some of which are
speculative, future possibilities.

\section{Background}

\subsection{The rise of personal data and services reliant on it}

As we pass through the digitally-augmented world that we collectively inhabit,
the set of actions with the potential to produce data grows year on year.
Portions of this outpouring are kept and stored as
\emph{capta}, from \emph{capere}: to keep~\cite{dodge2005codes}. From
using an access card to unlock a door, right down to tracking
individual footfalls, pervasive digital systems illuminate and annotate our
physical activity. Accreting around this body of physical observation is an
expanding sphere of mental observation and analysis. This can take the form of
active practises around recording mental states, such as journalling, but it can
also include computational inference, where frequency of posting on social
networks becomes an adjunct metric for connectedness, and search terms 
indicators of intent. As such, the modes of collecting this information can
range from explicit, user initiated submission of data, through consensual
background recording, to invisible, asymmetric electronic surveillance.

Increasingly, in order to utilise services, we must provide our data to third
parties. This ranges from mobile phone numbers being required for Yahoo
accounts, to location data being shared with Foursquare or Grindr, to the NHS
adding personal health information to centralised databases. 
The pervasiveness of computationally mediated action and interaction in modern
life means that for much of this data ``refusal is not a practical option, as
data collection is an inherent condition of many essential societal
transactions''\cite{brunton2011vernacular}. 

This leads us to introduce the term 
\emph{fiat data}---when an
organisation uses its position to demand (by fiat) the disclosure of certain
information in return for use of its services. 
Much as being a citizen of the United Kingdom requires paying taxes in pounds
Sterling, if one wants to interact socially on Facebook, one must pay the
personal data tax which they demand. 

In some cases, the provision of personal data is
necessary requirement for the provision of a service, but in others
it represents an attempt by the organisation to create a monetizable product from
its users. Many pieces of data fit both goals---for example the addition of 
mobile phone numbers to user accounts may have security benefits for the users,
but it also adds stronger, more persistent ties to offline identities.

When data is collected, there is a spectrum of approaches from outright demands, 
to asking or encouraging users to furnish their data.
Increasingly, \emph{gamification} is used to manipulate
users into self-surveillance, by providing rewards---whether within the system
or through the promise of self-improvement---for activities which require the
sharing of data to function:
\begin{quote}``Literally, within an hour of waking up, I am playing at least 
two games that promise to help me become a more productive
worker and prolific writer. \ldots I want to suggest two things: 1) that gamification is a form of
surveillance; and 2) this surveillance is
pleasurable''~\cite{whitson2013gaming}\end{quote}
Fitness apps, activity monitors, location based social networks require the
user to hand over their location data in return for the promise of increased
fitness, self awareness to the ability to connect with others. 
This user-driven data collection becomes a form of \emph{participatory surveillance}:
\begin{quote}
``Online social networking can also be empowering for the user, as the monitoring and 
registration facilitates new ways of constructing identity, meeting friends and colleagues 
as well as socializing with strangers. This changes the role of the user from passive to 
active, since surveillance in this context offers opportunities to take action, seek 
information and communicate.''~\cite{albrechtslund2008online}
\end{quote} 

There have been calls to create privacy enhancing tools which minimise the
amount of data that is collected to only that which is necessary. Data
minimisation, combined with user control over the data and preservation of the
context where the personal data was originally disclosed, make up the
\emph{Privacy 3.0} view of Borcea-Pfitzmann et al.~\cite{Borcea2011privacy}.     
As the ability of technology to record our lives evolves and improves, there is
an increaslingy acute need that the legislative side must catch up and provide
the framework for the recognition of personal information ownership rights~\cite{Schwartz2004property}.

There is a divergence between the goals of privacy conscious users and
organisations providing services based on personal information.
Spiekermann and Novotny have develope a tiered model for privacy-aware
information markets to elucidate the tension between these different
actors~\cite{Spiekermann2014}.
The model divides the market into four spaces: 
customer relationships, organisational control, customer control,
and safe harbors for big data. Each of these spaces has its own rules and
responsibilies for those operating within it, 
and recommendations from the authors on what technical and legislative
infrastructure is needed for their realisation.
Some of the most important issues and solutions they
present include the separation of the service exchange from the information
exchange, the mandatory privacy-friendly service option that providers must
offer, and the lawful accountability of organisations for the personal
information that they collect, including any subcontractors they might pass the
information to.                  

In summary, while there are many situations where we are coerced, cajoled or
manipulated into sharing our personal data, the cost of avoiding such
sharing is increasingly becoming untenable for large sections of the population
of the connected world. However, there are steps being taken in the direction of
a privacy-aware market for personal information, which protects the ownership
rights of users over their personal data, and their right to privacy, while also
allowing for the market to innovate and grow based on legally obtained and used,
and of higher quality personal information.    

\subsection{Issues with data sharing, control and the future}

Sharing data, by definition, is the entrusting of other parties with
information; this necessarily involves relinquishing control over how it is
subsequently handled and disseminated. While it is possible to attach terms and
conditions to the data at the point of sharing, the sharer must rely on the
either technological means or the compliance of the recipients. Technological
means to control information usage---such as Digital Rights\footnote{Often called Digital \emph{Restrictions}
management by opponents, e.g. \url{www.defectivebydesign.org}} Management
(DRM)---are generally quickly circumvented~\cite[p.6]{doctorow2008content}.
Research is being carried out into improving \emph{accountability} (e.g.
\cite{feigenbaum2011towards}), which can help to create a context which limits
breaches of trust. However, again, this relies on mechanisms outside the control
of the sharer, who must rely on legal means and the evolution of social norms
to ensure their desires are met.
Essentially,
once data is shared, the sharer has no control over what happens to it. 
Once shared, the data becomes persistent. Bruce Schneier commented\footnote{as
part of a panel at the ``Don't Spy on Us'' day in London, UK, on 7 June 2014}
that we are in the middle of a social change, where the ephemerality of
action is being lost:
\begin{quote}
``Google doesn't only know what I think better than my wife does, it
knows what I think better than \emph{I} do, because it remembers all I said and
did''.
\end{quote}
We are used to operating in a world where what we say will be forgotten
sooner or later, where actions remain embedded in the context in which they
occurred. The increasing persistence of record challenges this, as all
utterances and behaviour becomes permanently availble for later scrutiny.

 %\todo{eMax: Do we want to mention DRM here as a futile attempt to control how
 % data is used once it's shared?} 
 %\todo{DMR: I've added a little bit, but feel free to expand! I was looking for
 %a ref. on how all drm gets cracked, but couldn't find anything sensible}
 %\todo{There is so much more we can say right here about this.}  

There are many issues with sharing data, here we highlight four of them:
\begin{description}
  \item[Sharing is persistent, while situations evolve;] once data is shared,
  there is no technical means to revoke it. However, the context around its sharing and the
  organisations involved are subject to change. For instance:
  \begin{itemize}
    \item A government or other organisation may decide to share previously
    confidential data, as in the case of the recent \emph{care.data} fiasco in the UK;
    \item a company may be bought and its assets acquired---the purchase
    of Moves by Facebook and the potential for subsumption of location tracking
    into social network data raised issues around the terms and conditions of
    data handling companies;
    \item unseen events such as security breaches can expose vast
    swathes of personal data, or court proceedings may force private 
    communications to become public - the Enron emails still
    represent the largest publicly available corpus of private emails. 
  \end{itemize}


\item[Technology improves:] what is safe to share now may not be in the
future. Brad Templeton from the Electronic Freedom Foundation uses the analogy
of ``Time travelling robots from the future'': the information collected now
will be subjected to increasingly sophisticated analysis techniques as time
progresses, so the implications of sharing that information can be far beyond
expectations. For example, in the future, it may be possible to carry out facial
recognition on massive quantities of CCTV footage, and reconstruct the movements
of a large proportion of citizens. This corresponds to the surveillance
robots coming back in time and monitoring us now. In a similar vein: ``Would
you have liked to be gay 40 years ago in a monitored society? Or an enemy of J.
Edgar Hoover with modern tools in his hands?''~\cite{templetonWatched}. Sharing
data today cedes control to the entities of tomorrow, with their greatly
enhanced capabilities.

\item[De-identification is not a magic bullet:] data is often shared subject to
the condition that it will only be shared in an \emph{anonymised} or
\emph{de-identified} form. Whilst sharing de-identified data aids in privacy
preservation, it can also create a false sense of security, since it is
generally impossible to rule out the chance of \emph{re-identification}.
This might be through the set of data released, or through the fusion of
multiple data sources to provide higher discriminatory power. 
As a highly public example, Netflix challenged the public
to create a better recommendation engine, based on a corpus of anonymised
viewing histories. Subsequently, it was shown that many records within the
database could be identified by comparison with publicly available
sources~\cite{narayanan2008Deanon}, let alone access to other, non-public data.
Narayanan and Felten's recent report~\cite{narayanan2014Deidentification}
explains in a non-technical manner why de-identification of data remains
problematic. Some data is particularly resistant to de-identification: for
example, location data, where four data points
are enough for re-identification in many cases~\cite{montjoye2013Unique}.

\item[Databases can be joined:] as more databases of personal information
become available, whether publicly or privately, the possibility to match, join,
correlate, and share data increases, and the effects of single points expand well
beyond the environment in which they were created or shared. In short, data are
held in \emph{leaky containers}: ``data move freely between different sectors of
society with the result that information from discrete contexts, e.g.
private life, work life and shopping, are being mixed rather than contained 
separately.''~\cite[p.37--44]{lyon2001surveillance}. This leads to unexpected
and unforseeable consequences when sharing seemingly innocuous data points: we
are not in a position to know the outcomes of our actions.
\end{description}

%  \todo{What you are sort of getting at as ``rights'' to data and the lack of
% regulation thereof; there is no notion of data ownership in any legislative
% form, nor is there a notion of course of maintenance and tansfer of such terms
% of ownership - except in the most select of contextx, people have no legal
% ``right to even their own data''}

%% \subsection{The case for lying and the importance of anonymity}

\subsection{The beneficial uses of ambiguity and lies in social mediation}

Studies in Computer Mediated Communication (CMC) and agent-based modelling of
online communities have studied the use of deception both in real and simulated
settings, revealing a contrast between \emph{pro-social} forms of lying and
deception, and \emph{anti-social} forms~\cite{iniguez2014Deception}.
A wide range of pro-social kinds of deception have
been documented, primarily around coping strategies that people adopt towards
dealing with the demands and complexities of social mediation in increasingly
digitally-connected lives.            

The one strand of work pertaining to such coping strategies, is the exploration
of how people leverage the ``space of ambiguity'' created by
``low-bandwidth'' computer-mediated communications channels, such as SMS (text
messaging) or Instant Messaging, or even voice mobile phone conversations. Here,
ambiguity abouth the channel is used to facilitate the use of pro-social
deception for the purpose of efficient mediation.  
For instance, an individual might intentionally ignore an incoming
call, or blame poor reception (where there is none) to terminate a call
prematurely, in a situation where a person is stressed with other priorities, or
avoid talking to someone for another reason.  In work by Aoki and
Woodruff~\cite{Aoki:2005:MSS:1054972.1054998}, it is argued that such uses of
ambiguity are beneficial to the actors who use them, by providing effective
avenues for exercising control while simultaneously mitigating potentially
socially awkward or damaging situations, with a low social risk.  
As digital systems become increasingly   sensor enabled and
aware of people's activities in order to provide more ``real-life'' kinds of
interpersonal interaction, the space for ambiguity collapses. This reduces
people's ability to leverage ambiguity, potentially reducing 
individual autonomy.  
Aoki and Woodruff argue that it is essential for
designers to design communications systems  with some ambiguity left, so that
users retain flexibility around coping strategies for social mediation.

The second strand pertains to the use of ``butler lies'', which are a more
explicit use of deception. Butler lies are used for the purposes of simplifying
social channel mediation. For example, in order to terminate a conversation
without leaving the other party feeling unimportant, one might use the excuse
that ``I have a meeting with my boss'', when in fact, no such meeting 
exists~\cite{Hancock:2009:BLA:1518701.1518782}.  
Reynolds et al. found that butler lies amounted to 27.1\% of all
communications mediation messages sent by participants who texted partners and
co-workers~\cite{Reynolds:2013:BLB:2441776.2441862}. 
The purpose of such messages was often to mitigate social
awkwardness or reputational damage, and reduce attentional load. Additionally,
they were used independently of the nature of the relationship among
communicating parties.  Such lies were
particularly employed in awkward situations such as when a speaker 
wishes to convey a message that ``she is too
busy to interact with the recipient, but not too busy to interact with others''
\cite{Reynolds:2013:BLB:2441776.2441862}.  A detailed analysis of the effects 
of having the lies exposed showed that the potential emotional impacts were
overall small, and much less significant than for other kinds of lies.                    

The important role of these studies is to demonstrate the beneficial nature of
some forms of deception and witholding of information, as 
coping strategies to deal with the complexities of social coordination and
an increasing demand for communication. The goals of \emph{translucence} in
communication systems~\cite{erickson2000Translucence} are noble, but they must
be balanced against users needs for ambiguity and autonomy.

While lie maintenance required to avoid discovery may be trivial (``sorry, I'm
hungry, have to go!''), it becomes complicated as lies
extend over time, loose their ephemerality, and become woven into the social
fabric.
The ability to compare multiple accounts of history---especially once the time travelling
robots are involved---means that dissonance within the social
fabric is more obvious than weaknesses in a single thread.

% White lies can aid in social network growth: \cite{iniguez2014Deception}

% Most, if not all, social interactions 
% involve both strategic omissions and various kinds of lies and
% non-truths to manage the myriad conflicting social demands placed upon us. 

% Butler Lies: \cite{hancock2009butler}

% Translucence: ``What we say and do with another person depends on who, and how
% many, are watching.'' - \cite{erickson2000Translucence} \todo{This is important
% and should stay\ldots}

% Contrast with Transparent society \cite{brin1999transparent}; power imbalance
% between parties. \todo{Can ditch this if you want!}

\subsection{Managing multiple identities}

A natural part of online life is the ability to tailor the persona we present to
different communities and contexts. An individual may want to disclose certain
things to their professional colleagues, while presenting differently to friends
and family or non-mainstream friend groups. 
As an example, prolific content creators on Youtube have multiple online
personae for different kinds of content---``official'' channels, a personal one,
and sometimes channels for characters from their works~\cite{guy2014ConstructedIdentity}.

Pseudonyms are one way of representing personae. In~\cite{dalton2013Pseudonymity} 
Dalton describes pseudonymity in social machines as
a one-to-one or a one-to-many relationship---between the human and the
pseudonyms. It is possible for a person to be consistently known by a pseudonym
over several systems, or use many pseudonyms because they are easy to create and
to maintain. However, the many-to-one relation is possible as well, as
illustrated by Anonymous, or by Nicholas Bourbaki in the field of mathematics,
where the same pseudonym is used by a group. The group can evolve in time while
the pseudonym remains. 

Bruce Schneier called Facebook ``a collapse of context'' where the personae are
forcibly merged. The spreading and linking of data across multiple
sources---\emph{leaky containers}---contributes to this contextual collapse.
In particular, data which is rooted in physical fact provides multiple
opportunities for joining up otherwise separate databases. 
These data leakages affect users in a manner which is at once invisible and
corrosive to the construction and maintenance of personae.

\subsection{Verification and provenance as an alternative to sharing}
\label{sec:verification}

Sharing is a crude mechanism. Once data has been shared, the originator can no
longer exert control over it, and must rely on the behaviour of the recipient,
which as noted may fail to meet user expectations. As danah boyd
notes: ``Any model of privacy that focuses on the control of information will
fail''. This leads the teenagers that boyd studies to
engage in \emph{social steganography}, manipulating messages so that ``Only 
those who are in the know have the necessary information to look for and interpret the information 
provided.''~\cite{boyd2012Networked}. Strategies like this work when there is a
difference in understanding between the surveilled and the surveiller, and
collapses as soon as the comprehension barrier is removed.

Validation, however is a more subtle tool:
if a user's personal dataset can be made sufficiently questionable as to be useless on its own,
then locus of control shifts to the user choosing to validate parts of the dataset,
which can be performed in a more nuanced,
contextualised manner. If a user is the final arbiter of trust, they can decide
to i) sign parts of their record, so that it is verified public fact; ii)
co-sign it with another entity, so either can  also verify it but not anyone 
else;
iii) verify it through an anonymous channel, so that the entity to whom they
provide verification cannot propagate the claim further. This verification can
be carried out entirely separately from the data store itself, allowing for the
presentation of different datasets as valid  in different contexts, as well as
unorthodox methods such as using the Bitcoin blockchain to notarise datasets, so
that they can be verified in the future without revealing them as true at the
time. A conceptual move towards verification and provenance based
approaches underpins the obfuscation strategies outlined in the rest of this
chapter.

\section{Review of current approaches and tools} 

Privacy tools for end-users of the Web have focused on approaches to allow
people to cloak their originating location (IP address) and identity online, as
they access web sites that may be instrumented with code from any number of
advertising networks and tracker agents.  The most basic of such tools
constitute simple browser add-ons that explicitly block the execution and
downloading of web beacons, tracking pixels and other tracking agents, through
use of a dictionary-approach (of known tracking agents) (e.g.
DisconnectMe\footnote{DisconnectMe - \url{https://disconnect.me/}} or AdBlock
Plus\footnote{AdBlock Plus - \url{https://adblockplus.org/}}), or using adaptive
algorithms to infer cross-site tracking (e.g. Privacy Badger\footnote{Privacy
Badger - \url{https://www.eff.org/privacybadger}}).  Similarly, plugins such as
HTTPSEverywhere\footnote{HTTPS Everywhere -
\url{https://www.eff.org/Https-everywhere}} force the browser to communicate
with all web sites using encrypted HTTP, to prevent ISPs and other
intermediaries from eavesdropping on traffic between the end user's computer and
the end-user host.  The use of HTTPS also thwarts some deep-packet inspection
(DPI) based tracking methods~\cite{kumar2006advanced}---such as those employed
by public WiFi access points at international Starbucks coffee shops---which are
used to determine customers' interests and also to filter and re-rank search
results to benefit Starbucks.                     

In the middle have been a handful of application- or service- specific
anti-tracking tools, including TrackMeNot~\cite{howe2009trackmenot}, a plugin
which aims to reduce tracking and personalisation done by search-engines by
diluting the user's query within a flood of other, random search queries.
Similarly, CacheCloak~\cite{Meyerowitz:2009:HSF:1614320.1614358}, intercepts
requests for a user's location from various location based services, and returns
plausible nearby, but inaccurate, results.

Peer-to-peer systems such as Bittorrent, Freenet and others pose additional
challenges for privacy because a user's client typically directly connects with
a large number of other individuals' clients.  This means the client's location
(IP address) is typically highly visible within the network. However, in some
cases, depending on particular details, approaches like Tor (discussed
next) can be used to disguise location.
Work on disguising an individual's interests within this network---to promote
plausible deniability of intent---includes projects such as
SwarmScreen~\cite{choffnes2009swarmscreen} and
OneSwarm~\cite{Isdal:2010:PPD:1851182.1851198}.  Other approaches, including
encrypting all transferred queries and data have been taken by systems such as
Mega\footnote{Mega - \url{www.mega.com}}.

Perhaps the most sophisticated current tool developed for obscuring network
activity and traffic origin is currently Tor~\cite{dingledine2004tor}, which
works at the network-level to hide the origin of packets when communicating with
a website or other third party.  Tor works as a peer-to-peer overlay network
that routes Web and other network requests through a randomly selected circuit
of hosts on the network using the onion routing technique, which makes it
intractable to deduce the origin of a particular packet.  In conjunction with
end-to-end encryption (such as over HTTPs), Tor has been shown to effectively
thwart eavesdropping and DPI methods.

Even with such tools, ISPs, mobile broadband providers and other ``last mile''
internet access providers can collect a significant quantity of low-level data
pertaining to the physical place and times that a person accesses sites, the
quantity of data exchanged, and the potential destinations.  To thwart this
level of tracking, several hardware vendors and operating systems are starting
to incorporate the ability to perform identifier randomisation (such as
MAC-address and Bluetooth hardware address randomisation\footnote{Such features
are expected to be introduced as a standard feature to the consumer market for
the first time by Apple in iOS 8 -
\url{http://appleinsider.com/articles/14/06/09/mac-address-randomization-joins-apples-heap-of-ios-8-privacy-improvements}}.
 Such features are standard for an emerging group of speciality \emph{security
hardened} devices, including security-enhanced mobile phones and tablets (such
as the Black Phone\footnote{Black Phone - \url{https://www.blackphone.ch}}).

While these tools are dedicated to obfuscating action and identity for actions
in the digital realm, automatic tracking of people and their activities in the
physical world has also increased, thanks to improvements in facial recognition,
the introduction of digital tokens and keys for granting people access to
physical spaces, membership cards (``loyalty programmes''), credit cards, and so
forth.  A number of projects and approaches have similarly been introduced to
reduce trackability in this space.  In terms of protection against facial
recognition algorithms, for example, Harvey et al introduced a special make-up
technique based on reverse-engineering the most popular face detection
algorithms, called CV Dazzle~\cite{harvey2012cv}. To reduce infrared camera
based person tracking Harvey also introduced heat-signature cloaking burqas and
hoodies in a line of clothing called \emph{Stealth Wear}\footnote{Stealth Wear -
\url{ahprojects.com/projects/stealth-wear}}.

To reduce city-wide tracking using transport cards such as the London Oyster
card, NYC MetroCard, as well as  purchase tracking via loyalty card schemes, a
common practice that has arisen in several of these cities has been to host
social meetups where people regularly gather and swap their valueless cards. 
This allows them to confound data mining algorithms by eliminating the
assumption that a single card will be owned and used by a single person, but instead by a
constantly evolving group~\cite{lockton}.

% \todo{Apple patent on cloned identities of the principal user - 
% http://www.forbes.com/sites/andygreenberg/2012/06/20/apple-patents-technique-tha
% t-uses-cloned-doppelgangers-to-protect-your-privacy/}
% 
% \todo{for full disapearance act:  How to Disappear: Erase Your Digital 
% Footprint, Leave False Trails, and Vanish without a Trace by Frank M. Ahearn and 
% Eileen C. Horan. 3 chapters on disinformation and creating false trails}

% \todo{Refs to work in\ldots}
% \begin{itemize}
%   \item Marwick, public domain: \cite{marwick2012Public}
%   \item Reigeluth, data traces: \cite{reigeluth2014Data}
%   \item Beer, algorithms and power: \cite{beer2009Algorithm}
%   \item Goldberg, public/virtual participation: \cite{goldberg2010Rethinking}
%   \item Simon, panopticism \cite{simon2005Panopticism}
% \end{itemize}

% \todo{Max to do some writing}

% \begin{itemize}
%   \item The revolution has started!
%   \item tor, anonymous remailers, burner phones, gotta change up, yo!
%   \item HTTPSEverywhere
%   \item Surveillance and Society: \url{http://library.queensu.ca/ojs/index.php/surveillance-and-society}
%   \item CVDazzle
%   \item Heat-signature cloaking burqas, hoodies
%   \item Bluetooth and MAC randomisation in iOS 8
%   \item Silent Circle, Cryptocat
%   \item DNT in IE10
%   \item Adblock/Adblock Plus, Privacy Badger, Disconnect.Me
%   \item HTTPsEverywhere
%   \item VPNs 
% \end{itemize}

% Open source and trustworthiness
% Theory of obfuscation:
% Types of disinformation \cite{alexander2010Disinformation}:
% \begin{description}
%   \item[redaction] is hiding some or all of the information in a message
%   \item[airbrushing] is changing some of the information. \emph{local crowd
%   blending} means change it to a nearby message likely to be plausible.
%   \emph{global crowd blending} means change it to a message in a dense part of
%   the space.
%   \item[curveball] add extra distracting information, push message into low
%   density space
% \end{description}

% Some existing stuff and the things we can link it to later

% \begin{itemize}
%   \item TrackMeNot generates plausible google searches (Chaff)
%   \item FaceCloak? Encrypts facebook data
%   \item CacheCloak - sends a range of plausible future paths to location based
%   services (Palimpsestification)
%   \item Shopping card loyalty swaps (Account Sharing)
%   \item DuckDuckGo - mixing up user searchers (Account Sharing, no cleverness)
%   \item CVDazzle
% \end{itemize}





\section{A selection of obfuscation strategies}
\label{sec:strategies}


The aim of this chapter is to set out some strategies by which the user can add
some disinformation into digital social networks, in order to regain some
measure of privacy. Since every situation is different, from the operation of
the network to the desires of the participants, it is necessary to develop
several strategies, and understand under what circumstances they may be
appropriate, and what tensions between utility and privacy arise from their use.

As a starting point, Alexander's taxonomy~\cite{alexander2010Disinformation}
discusses several types of disinformation which relate to modifying single messages along schema such
as:
\begin{description}
  \item[redaction], where some or all of the information in a message is
  hidden;
  \item[airbrushing], where some of the information is changed. This can take
  the form of \emph{local crowd blending}, where small alterations are carried
  out so the new message is similar enough to be plausible, or
  \emph{global crowd blending} where messages are heavily altered in order that
  they resemble plausible, but quite different messages.
  \item[curveball], where extra, distracting information is added to messages
  which pushes them into a low density area of message space.
\end{description}
However, due to
the pervasiveness of modern communications, we are concerned with modifying
message \emph{streams}, where a trace of multiple values must be considered.
The social aspect inherent to modern communication tools increases the 
range and frequency of interaction with others, which in turn increases the 
chance of lies being exposed, while at the same time opening up the possibility
of colluding to strengthen the obfuscatory practices. 

In this section, we present a range of obfuscation strategies, some of which are
speculative, but many of which are drawn from existing examples both inside and
outside the digital sphere.

For each strategy we discuss: \begin{inparaenum}[\itshape i\upshape)]
\item what kind of alteration of baseline data is performed;
\item what is the motivation for doing it; 
\item what are the possible use cases;
\item how some form of computational support aids in the deception;
\item some of the systems (if any) which do this currently.
\end{inparaenum}

It is problematic to consider the obfuscatory tactics here without a sense of
the scenario in which they are to be deployed. Our scenario in this chapter is:
\begin{quote}
The user wishes to make use of services which expect location information; 
the location information provided is shared publicly and is almost
certainly stored indefinitely. At times, the user may want to draw on location
based information---such as restaurant recommendations or directions---and there
may be times when they wish to verify that they were at a particular location.
\end{quote}
The service is hence \emph{semi-trusted}: there are some benefits which the user
wishes to accrue, but there are aspects of the service which makes the user
unwilling to entrust their complete location history to it. We have chosen location
as a clearly understandable facet of personal data, and one which can be easily
used to re-identify individuals from anonymised
datasets~\cite{montjoye2013Unique}.

\fig{Mediation}{Models of interaction with semi-trusted services. a) Direct
transmission of information; b) computationally mediated transmission, where 
some friendly computational machinery is enlisted to aid in
obfuscatory processes.}

The standard model of interaction (Figure \ref{fig:Mediation}a) involves the
user submitting their data directly to the service; for our obfuscatory
techniques, we would like to enlist computational support (Figure
\ref{fig:Mediation}b). This typified, but not limited to mediation from a
personal data store which acts on behalf of the user to modify the data which
they provide.
In Figures \ref{fig:SinglePlayerObfuscation} and
\ref{fig:MultiPlayerObfuscation}, we plot a fictitious one-dimensional
`location' measurement against time in order to give a sense of how
obfuscations unfold across time in multiple locations\footnote{While a two
dimensional, map-based representation would be more immediate, it is then
difficult to clearly show temporal aspects.}. We show the individual's true `location' as
a continuous line, along with the measurements made by their device; we then
overlay the points which would be submitted on their behalf after obfuscation.

\subsection{Strategies for the lone obfuscator}

\fig[0.91]{SinglePlayerObfuscation}{Obfuscation strategies for the lone agent.
Location has been reduced to a single dimension so it can be plotted against time.
Real data is shown in purple with circular points. Constructed or manipulated
points have different shapes. Schemes which requite a simulation of human
behaviour are marked with a head icon
(\includegraphics[height=7 pt]{figures/Head}), and where an alternative
model of the world is created, it is shown as a dotted line. 
}

Figure \ref{fig:SinglePlayerObfuscation} lists a collection of possible
obfuscation strategies, with Figure
\ref{fig:SinglePlayerObfuscation}a showing the true location as a curve, 
and the dots representing reports of this position to the location-aware service. 
% For each strategy we discuss: \begin{inparaenum}[\itshape i\upshape)]
% \item what kind of alteration of baseline data is performed;
% \item what is the motivation for doing it; 
% \item what are the possible use cases;
% \item how some form of computational support aids in the deception;
% \item how the strategy can be applied to other data
% \item some of the systems which do this currently.
% \end{inparaenum}

\subsubsection{Chaff}

World War II fighter planes would emit clouds of radar reflective
sheets---\emph{chaff}---which created multiple traces the screens of radar
operators, and hence disguised the true position of the aircraft. In a similar
manner, we can add in multiple location data points alongside the real ones,
making it difficult to determine what the true values are.
This is the one of the few methods where the complete, accurate data stream is stored.
Hence the user can still access any benefits which rely on accurate information. However,
adding a multitude of randomised points to a service which expects a single
contiguous trace is both easily detectable, and may break functionality---a
run tracking application would be likely to give unreliable distance estimations
in the presence of chaff.

\subsubsection{Noise injection}

The most computationally simple form of obfuscation is the addition of noise to
the reports which are sent to the semi-trusted party. Here, the points which are
submitted deviate from the true values in a random manner. This allows the user
to conceal their exact location, while giving a broad indication of where they
are. Depending on the level of noise, this can allow the use of location
based services without revealing much about actual behaviour. For example, it
might reveal your location on the high street so you can arrange to meet
friends, without revealing which shops you were visiting. This is compatible
with services which expect coherent location data, and may be indistinguishable
from the inaccuracies of the location sensors. One downside is that the ``true''
location traces are not present in the record of the service.
% \todo{examples?}
For example, TripAdvisor can still provide a good enough list of recommended 
attractions around the given ``noisy'' location, however a navigation 
application will not be able to provide reliable directions.

\subsubsection{Coarsened Granularity (or Quantisation)}

Rather than adding noise to the data being sent, it can instead be quantised to
a coarser granularity, akin to blurring, or zooming out on a map. Again, this is
a technique which may help to derive useful information from the service,
without revealing more than is necessary: using a service to find friends in the
same city should only require city level information to be shared. An example of
this can be found in Android's permission system, which has separate controls
for \verb|ACCESS_COARSE_LOCATION| versus \verb|ACCESS_FINE_LOCATION|.

\subsubsection{Systematic Deviation}

In some cases, it may be possible to introduce systematic deviations into the
digital record. In order to do this, the user needs to be able to define which
points to alter, and what to replace them with. One possibility would be
thematic replacement---``hide the times I went to the pub by saying I was at a
cafe''. Another would be to disguise the user's home and work locations---places
where they are less likely to require location based searches, but which make it
very easy to re-identify them from anonymised data.
It is likely that this will require some form of computational support to i)
identify targets for replacement as they occur and ii) find suitable
replacements.
Using this technique, some, but not all of the true data is stored; however
derived information---such as beverage preferences in the  example above---can
be wildly and purposefully distorted. The nature and fact of
the distortions may be hard to uncover, as no simultaneous traces or strange
movement patterns are produced. Depending on the domain, subtle alterations may
have large effects.
% \todo{examples?}

\subsubsection{Pretend to be me}

With increasing computational support, it becomes possible to create a model of
the user which outputs plausible `normal' data. This can then be used to
replace periods of abnormal behaviour, or even replace normal behaviour with
statistically similar but untrue data. An early example is when neighbours (or
automatic switches) are employed to turn lights on and off in a home which has
been vacated for the holiday, disguising the true anomalous data of a dark,
empty house with the appearance of normal occupation. Similarly, one might avoid
making Facebook posts which indicate an absence, to avoid burglary. This kind of
deception can be difficult to achieve; however computational systems are
emerging which can aid users, for example Beyer's digital alibi system
\cite{beyer2014Alibi}.

\subsubsection{Coherent Deviation}
As the converse of simulating normality, the user may wish to pretend to be
somewhere where they are not. 
% \todo{more motivation for Alibot!} 
This is similar
to creating systematic deviations, but on a grander scale; the user would like
to create a narrative for the deviation, and then have suitable data points
constructed. For example, the user might pretend to be on holiday, or at a
conference, and would like location traces which match that narrative to be
created, such as going to the convention centre in the day, and returning to a
hotel at night. This requires a computational model of user behaviour which can
be applied to new locations---a non trivial task. However there is the potential
to create obfuscated data which is difficult to distinguish from standard
behaviour. Services like Please Don't Stalk 
Me\footnote{\url{http://pleasedontstalkme.com/}} enable geotagging tweets 
with fake location information, and the Chrome browser supports faking location 
information in the browser, through the Chrome Developer Tools emulator. These 
tools can be used for some of the previous strategies, for noise injection by 
using random locations, or for systematic deviation, when only the location of 
certain tweets is modified.

\subsubsection{Palimpsestification\footnote{`Palimpsest' originallyer referred
to a document where orignal markings have been abraded so that it may be
re-used, but still contains some traces of the original, so the texts are
superimposed. Modern usage covers a range of other situations when multiple
activity traces are overlaid.}}

 Taking the idea of coherent deviations a stage further, and combining with the idea of 
\emph{chaff}, the user could create multiple overlapping traces; 
each trace would be locally coherent and plausible, but someone inspecting the 
data would have no way to know which is the real one.
This is similar to the strategy of 
CacheCloak~\cite{Meyerowitz:2009:HSF:1614320.1614358}, which
continuously generates sheaves of probable future behaviour and searches
location based services relevant to each  path. The computational support
required is similar to the coherent deviation example---to be able to run a
model of the user's behaviour in novel locations---although more coordination
might be required between the stories. The trade-off is that while the true
location data can be entered along with the generated points, the deception is
obvious, and location based services may become upset at the multiple paths.

\subsection{Collaborative Obfuscation}
\fig[1.0]{MultiPlayerObfuscation}{Three possible multi-person obfuscaction
strategies. As before, modelled behaviour is shown as dotted lines. In the \emph{Artificial
co-location} strategy, circles represent real datapoints, diamonds are the ones that the
obfuscating user submits. In the \emph{Account sharing} strategy, the shape of
points represents the account through which they are submitted. }

Including others in the obfuscation challenge opens up a range of new 
strategies, where collusion can aid in the creation of otherwise unachievable
data streams, or increase the veracity of artificially created data. Generally
the possibilities in computational systems are analogous to pre-computational
possibilities; computational support tends to be in the form of coordination to
find collaborators or and check coherence of data points. The ideas outlined
here are more speculative, as few computational systems of this type exist.
There are aspects which make these strategies harder to pull off as coherence is
required across multiple different accounts; however the counterpoint is
that if successful, the obfuscation is better supported and harder to detect.

\subsubsection{Artificial co-location}
One way to obtain a realistic but untrue location trace is to re-present the
trace of a collaborator. This can look like relatively natural
behaviour; two people meeting up to carry out joint activities or socialisation.
Computational support here can involve finding accomplices to ``co-locate''
with---people who are willing to share their location, and are behaving in ways
which match the desired story---as well as the technical details of
transferring location devices between accounts.

\subsubsection{Supporting Evidence}

% \todo{Maybe merge into preceding co-location section}
Co-located people often share the fact of their co-location, explicitly or
implicitly, whether in group photos---``Here's me and X on top of the Scott
monument''; broadcast messages---``Just been hanging out with X at the coffee
shop''; or shared plans---``Going to the cinema with X tonight - anyone want to
join in?''. Enlisting collaborators to make these kinds of posts can help to add depth to a
constructed trace, weaving it more tightly into the social fabric.

\subsubsection{Account Sharing}

In a similar manner to the swapping of loyalty cards discussed in
\cite{brunton2011vernacular}, users of services can share accounts. This
results in an account or set of accounts with more or less plausible activity,
yet allows the users to remain unidentified. 
Much as the loyalty card swapper confounded efforts to inspect individual
buying habits, or the ``Anonymous'' movement aggregates the activities of a
multitude of participants behind a stylised mask, services such as DuckDuckGo
aggregate many people's search results, ensuring that the search providers
cannot build up any identifiable user histories. 

Computational intelligence can be enlisted to support many different ways of assigning people to accounts,
 such as:
\begin{description}
  \item[\emph{Many to one}] schemes have a single account controlled by
  multiple people. This can result in a completely incoherent manner;
  DuckDuckGo's aggregated search makes no attempt to imitate
  individual behaviour. 
  Alternatively, sharing can be tied into a coherent shared identity,
   where multiple people contribute to a single shared
   persona~\cite{dalton2013Pseudonymity}.
   Here there is a challenge to maintain consistency: when multiple people
   control a call centre's chat avatar, they must ensure that the relevant
   information and state is shared [\emph{ibid.}]. When multiple users control a
   single game character, the game world enforces consistency, and the community
   must produce coherent action streams in order to progress.
%    \todo{cite Twitch!}.
   \item[\emph{Randomised}] schemes allocate accounts to people without a
   discernible order; when loyalty cards are mailed between anonymous
   participants, there is an explicit desire to produce implausible data in
   order to confound analysis. Online accounts can be similarly shared, leading
   to traces which are unlikely to have been produced by a single individual.
   In our locative service example, this allows users to access benefits which
   do not rely on individual history, while preserving some level of privacy.
   Computational support involves finding accounts to share, and ensuring that
   each account is only accessed by a single person at any given time.
   \item[\emph{Structured}] schemes allow for accounts to be used as appropriate
   according to some criterion. If a location service offers history based
   benefits (e.g. loyalty rewards or reputation) then it could be beneficial to
   borrow a local user's account when going on holiday---couchsurfing but with
   login credentials instead of flats. Infrastructure would be required to
   discover appropriate accounts, and mediate access. 
%    \todo{More explanation? Link to powerlevelling in WoW?}
\end{description}

\section{Operationalisation - managing deception and its side effects}

\subsection{Going beyond location}

In Section \ref{sec:strategies}, we discussed obfuscatory possibilities with
respect to a location based service; however, this is a single application area,
and the need for regaining informational autonomy is felt across spectrum of
data types and services, hence we must discuss how these techniques generalise.

Location data is generally collected by a device which the user owns. In many
cases, this is a smartphone, which uses a combination of GPS, cell tower
triangulation and WiFi access point locations to determine a user's position in
space. The user then has some level of choice about who to share the data with
and how. This is not always the case, however: cell tower records can identify
user's locations---and individuals can be picked out from very sparse
histories~\cite{montjoye2013Unique}. There was a recent 
outcry\footnote{O'Reilly Radar:\url{
http://radar.oreilly.com/2011/04/apple-location-tracking.html}} when it was
discovered that smartphone manufacturers were collecting location data from
their users without their knowledge, despite the possiblity of good intentions
or technical reasons\footnote{Apple's
response:\url{http://www.apple.com/pr/library/2011/04/27Apple-Q-A-on-Location-Data.html }}.
% \\ArsTechnica: \url{ 
% http://arstechnica.com/apple/2011/04/how-apple-tracks-your-location-without-your 
% -consent-and-why-it-matters/}, \\Wired: 
% \url{http://www.wired.com/2011/04/apple-iphone-tracking/}, \\WSJ: \url{ 
% http://online.wsj.com/news/articles/SB10001424052748703983704576277101723453610} 

Obfuscating data (location or otherwise) can be done only if the user is
aware that the data is being collected in the first place. Some knowledge
about the data collection process, such as what data is captured, when can
then help improve the obfuscation.

Some data, even when known to be collected, cannot be obfuscated. Examples 
include all official or administrative data: census data, police reports,
tax returns, health records, but also dealings with private organizations with a
regulated status like banks and utility providers. 
Obfuscating these types of data in any way is often a a criminal offence. 
For example ``obfuscating'' one's income source by adding chaff or noise may be
construed as tax evasion, theft, money laundering, etc.
% ; obfuscating energy consumption data can lead to suspicion of growing marijuana 
% (in some places), or running illegal servers (in other places). 
% \todo{do we want to go further?} Even weirder, obfuscating family membership :) 
% false number of children / parents / supported or supporting family members. 

For data that can be obfuscated without legal consequences, and for 
which we are aware of the data collection, we have the opportunity to use some
of the strategies described in Section \ref{sec:strategies}, although they must
be adapted to fit the data and the situation at hand.

For a photo sharing application or microblogging tool, adding \emph{chaff} 
could mean posting more content, but fabricated. 
It can be realised by posting public domain photos on Instagram from a randomly 
chosen topic, or flooding Twitter with randomly generated text, older posts, or 
fragments from existing texts like web pages or books. For browsing history in 
an online shop, we can use bots or browser extensions, 
or tools like Selenium\footnote{\url{http://docs.seleniumhq.org/}}
to simulate browsing of various items by randomly requesting 
pages beside the page we are interested in. While it does 
not hide the fact that a person visited the shop, it can obfuscate their interest. 
This can be useful when dealing with Amazon, or another large scale marketplace, 
but it can be less useful when the online shop is catering for a small niche 
market. More subtly, chaff can be added to the tags attached to photos or posts,
to make categorisation more difficult. 
The goal of chaff is not to be believable
on deeper inspection, or even consistent, so the algorithms for choosing 
what to post and where to browse need not be too complex. 

Prepaid, disposable mobile phones (`burners') are used in order to avoid
surveillance, as they break the link between a person's identity and their
cellphone number.
Similarly, ghost email addresses can be used to obfuscate online activity, as it
prevents the email address from being used as a global identifier for the person. For websites which 
require registration, 
disposable email provider services\footnote{A comparison between 
some disposable email providers is available on LifeHacker 
\url{http://lifehacker.com/5306452/how-do-you-keep-your-email-address-private}}, 
mean that a different email address can be used for each site, while still 
receiving the email confirmations in a `real' mailbox. These email addresses can 
still be linked back to the true user if the service is compromised 
in some way; for truly disconnected disposable email addresses, services like 
GuerillaMail, 10 minute mail, etc. provide temporary email addresses with a 
short life span. 

Applying \emph{noise injection} or \emph{coarsened granularity} strategies can 
limit the benefit of some services, and in some cases defeat the purpose of 
using them altogether - like injecting noise in the images posted on Instagram.

These strategies can however be used successfully with optional data that 
is required by some services but do not impact on the service provided. 
Randomly changing your date of birth or hometown, or even your name on Facebook 
or Twitter will still allow you to use the service to connect with friends or 
post updates respectively. Changing (injecting noise into) the date of birth 
can be used as the real date of birth approaches, to avoid the 
congratulatory messages, and reverted back once the date has passed. A 
different type of changing granularity is withholding the year from the 
date of birth, thus allowing the actual date be know, without revealing the 
age. Any functionality of a service that is regarded of low importance can be 
used as a place where noise can be injected, or where the granularity can be 
coarsened. This of course leaves part of the information unobfuscated. 

Changing the declared date of birth can be also \emph{systematic}, a user can 
choose an untrue date which to reuse in \emph{all} services that require it. 
Using the same completely fabricated persona in all online communication is 
supported by services like Fake Name 
Generator\footnote{\url{http://www.fakenamegenerator.com/}} which will not only 
create geographically localized names, but also provide valid additional 
information like address, credit card, email address, phone number, mother 
maiden's name, username and password, employment information, weight and height 
and blood type, and even a favourite colour. Using the persona systematically 
and consistently enables a complete separation between the real person and their 
presence online. 

Several personae can be created and used to simulate collaborative obfuscation 
strategies, where the fake characters mimic the existence of supporting 
evidence for the lie. \emph{Astroturfing} uses multiple accounts to simulate
widespread support for a topic, organization or political message, 
creating the illusion of a grassroots movement. It can however be used for
the obfuscation of individuals' data, by corroborating claims, and adding depth
to the constructed scenarios created.

\subsection{Personal Data Stores - allies on the intimacy battleground}

Personal data stores (PDS) represent a vital component in the issue of online
presentation: having trusted, user controlled repositories for data
enables a more user-centric approach to management of \emph{capta}, filling in
the `Mediator' component of Figure \ref{fig:Mediation}. Bridges can then be
built between personal data stores and the rest of the world in order to support 
the connected, networked interactions
which users now expect. If these bridges simply share the data, even in a
controlled manner, nothing has been gained; hence the bridges become conduits
for manipulating truth and constructing falsehoods. As personal data stores
accumulate more real-time contextual data about the individual, as well as about
the individual's social connections, PDSes can provide support for the often
stressful and mentally burdensome task of lie maintenance, for example: i)
identifying when a person's real activities or whereabouts contradict a lie, and
might be discovered; ii) identifying indirect social channels that could expose
a lie (e.g. through friends of friends); iii) suggesting appropriate lies to use
which are least likely to be detected; iv) suggesting individuals to lie to to
support lie maintenance (e.g. friends of the person being lied to). 


\subsection{Verification and provenance mechanisms}

\fig{Verification}{Example verification scenario. The user (Ally) provides a set
of real data, plus \emph{chaff} to a location aware service. A third party
(Brett) then requests verification of some of the points, which Ally
provides. Brett then wishes to share the data with Charlie, which requires Brett
to verify to Charlie that the data are correct.}

In Section \ref{sec:verification} we suggested that verification is a more nuanced mechanism
than control over sharing.
One of the effects of the obfuscation strategies discussed
previously is that it becomes impossible to know which parts of the user's
data-stream are grounded in reality, and represent ``true'' values. This means
that if someone wishes to engage with the data and have an expectation of
accuracy, they need to ascertain which parts of the record are correct. This
shifts the locus of control from the process of sharing to the process of
verification---the user can make claims about subsets of the data points
currently attributed to them.

Let us consider a scenario where Ally has some personal data, which Brett would
like to make use of. Brett also wants to sell Ally's data to Charlie.

There are a range of statements which Ally can make, 
including:
``this subset of data points is mine'',
``these points are within 50m of my true location'',
``these points are representative of my general behaviour'' and so on. 
The choice of which point to claim can be negotiated
in the context of the question being asked, and Ally can determine what is
and is not acceptable.

If the external agency wishes to disseminate the users data, it becomes an issue
of propagating the trust which the user has given them---essentially, Brett must
say to Charlie: ``Ally has verified these points to me, and now I am
verifying them to you''. The manner in which the initial verification was carried out now becomes
critical:
\begin{itemize}
  \item if an email or similar communication is used, Ally simply declares
  ``these points are mine'', then the secondary verification is only as strong as
  trust in the communication chain---Brett must convince Charlie that the email
  or message is genuine and emails are easy to fake;
  \item Ally could use a technique which would give Brett no future tangible
  proof of the verification---for example, a single use URL which
  lists IDs for the correct points. Brett would have no evidence with which to
  convince Charlie that Ally had verified the points, other than reputation
  alone.
  \item Ally can cryptographically sign the claim using public key cryptography.
  The claim is then essentially public knowledge, and anyone can check Ally's verification.
  \item Ally can sign the claim after Brett has; this means that Brett cannot
  hide the fact that they were the recipient of the claim, so it is impossible
  to propagate the claim anonymously.
\end{itemize}

All of these techniques relate to making the public record so unreliable that
anyone who wants to use any of the data will need to separately establish a
chain of provenance for certain parts of it. A related goal would be to make it
illegal, or at least unacceptable, to use personal data without having a valid
provenance chain for it. Essentially, in order to use anyone's data, Charlie
would have to explain how they came to have it, and be able to prove that Ally
had shared the data originally.


\subsubsection{Notarization}
\label{sec:notarization}
\fig{Notarization}{Notarization of personal data. a) Data points and times are
hashed, and the values sent to a notary service, which provides a URL to verify
that i) the given data was supplied and ii) when it was supplied. Hashes are
used so that the data is not publicly shared. b) If the hash of the previous
submission is included, then sequences of consecutive points can be verified.}

The verification examples above rely on Brett trusting Ally about which
data points are correct. There may be times---e.g. when creating alibis---when
Ally needs to have a stronger form of proof.

In this case, third party digital notarization services can be
employed\footnote{e.g.
\url{http://virtual-notary.org/}, a free service hosted at Cornell University}.
These services take in some document or datum, and provide a certificate which
can be used to verify that that piece of data was provided at a certain time. 
For example, if someone wants to make a prediction
for the outcome of a football match, they could notarize that before the match,
and then subsequently prove that they had made the prediction beforehand. It is
generally not possible to prove that they only made a single prediction, so this technique is most suitable when the range of possible things to notarize
is so large as to make notarizing the entire space infeasible.

With regard to personal data, we can notarize our true data stream as we produce
it. This means that we can prove that we had considered those points at the
time, and if we say we were in a particular place, there is a high chance we
were---however, it does not work in the complementary situation as producing a
notarized point does not prove that we were not anywhere else.


Notarization does not necessitate revealing the data itself. For instance,
when submitting a location, a representation of the time and place could be
hashed, and this hash notarized (Figure \ref{fig:Notarization}a). Additionally,
points can be notarized in sequence, so that we can demonstrate contiguous sub-sequences 
of points as having been provided previously; by
hashing the current location with the previous location, we can link the points
together, to build up confidence in the notarized results (Figure
\ref{fig:Notarization}b).

\subsection{Effects and ethics of obfuscation}
% \todo{How do services react when we stuff them full of noise?}
% \todo{Is it OK to do this stuff?}
Brunton and Nissenbaum discuss at length the ethics of obfuscation in
\cite{brunton2011vernacular}, and address issues like wastefulness, dishonesty,
free-riding, pollution, and possible system damage. One answer is ``that
obfuscation has no ethical or political valence of its own, only to the ends
that it serves'' [ibid.]. However, with the sharing of personal data, there are
multiple overlapping contexts in which the obfuscation takes place, which should
be dissected.

On a \emph{personal} level, obfuscation changes the picture of us that a service
has. For different services, different levels and types of distortion may be
bearable or desirable: while adding noise to a location signal may still allow
sensible location based recommendations, pretending to like random products
might make a recommender service unusable. 
 One of the reasons so many obfuscation strategies were presented is
that each strategy preserves different qualities, and the choice of what and how
to obfuscate must be made in context, as there is an unavoidable tradeoff
between privacy preservation and maintaining the level of personalisation that
makes a service worthwhile.

On a \emph{social} level, being caught in a lie can be unpleasant; as
Bok says:
\begin{quote}
``In practice, however, lying to enemies has enormous drawbacks...  lies to
enemies carry very special dangers of backfiring. All too often, the lie
directed at adversaries is a lie to friends as well; and when it is discovered,
as some always are, the costs are high.'' \cite[p. 141]{bok1978lying} 
\end{quote}
While the reputational damage from lying can be large, some steps can be taken
to mitigate this. Obfuscation techniques can be chosen which are
clearly non-true to human observers, or to one's trusted circle of friends,
echoing boyd's \emph{social steganography}~\cite{boyd2012Networked}. The
alterations will then have the desired effect of confounding automated
surveillance, without giving one's friends the sense of being deceived.

Additionally, systematic data manipulation can result in presenting an online
persona which diverges from one's own. Recently, Mat Honan tried the experiment
of ``liking'' everything on Facebook~\cite{honan2014Facebook}. After two days of
adding this \emph{chaff} to his activities, his public persona demonstrated
radical far-right and far-left viewpoints, to the extent that his friends became
concerned.

As well as presenting an untrue picture of oneself, systematically adding noise
may also have unplanned side effects that might directly impact one's friends and connections. 
For example, Mat's friends suddenly found themselves flooded by the stream of liked articles he was 
generating, to the point where his junk data stream overwhelmed others' friends' legitimate statuses
and impeded their ability to use the social network normally. 

On a \emph{societal} level, while obfuscation overlaps both `pro-social' lying
and `butler lies', it must also be recognised that systematic mistruths can weaken the social fabric,
disrupting trust with friends and coleagues, as well as those
carrying out surveillance. This adds friction to online interaction, as one must
put the effort into constantly questioning what data is true and false.
On a broader scale, the social good which comes of having access to increasingly
detailed personal data can be compromised if significant proportions of the data
are untrue. 


In terms of \emph{accountability}, there is the danger of being unable to
verify your real behaviour. 
While the possiblity of notarization (Section \ref{sec:notarization})
can create support,
any defense which relies on a stream of information
which has been systematically manipulated is likely to be problematic. As online
systems are increasingly being used to account for who you are and what you do,
making your online persona questionable could have far reaching consequences. A
complementary risk is that the constructed data might be accidentally
incriminating: a spuriously generated location point might place a user at a
crime scene, or a participant in an account sharing program could use a
borrowed account while engaging in criminal activity.
There is also a danger that the same tools that preserve personal
privacy are applicable to covering up illegal or anti-social behaviour (e.g.
cyber-bullying). 

Finally, at a \emph{systems} level, the context is dynamic, and services will
respond to obfuscatory practices. 
Services may shut down or transition to a payment based model in response to the declining
value of increasingly noisy data.
Future services will be better at spotting 
obfuscated data, and may take steps to prevent it, analysing the coherence and
plausibility of incoming data. Obfuscation systems will have to evolve along
with the context in which they are used, creating an arms race between
obfuscators and service providers.

These issues suggest firstly that obfuscation is a form of \emph{free
riding}~\cite{brunton2011vernacular}: if everyone did it the system would grind
to a halt, but if a few people do it, they can take advantage of everyone else's
truthfulness to preserve their own privacy. However, if the majority of the
population engaged in obfuscation, this would be a clear signal that the balance
of privacy was unacceptable to the general population, becoming a voice of
protest.

Secondly, it is clear that more understanding is needed around what kinds of
obfuscation to apply when, and how to create tools to enable these practices in
the context of connected, visible, online social behaviour.

However, there is a view that anonymity is natural and healthy
both on and off-line, and that methods that force full identifiability at each
interaction are both difficult, fragile and unhealthy for long term growth. 
Proponents of this view (e.g. 
\cite{boyd2012Networked,pseudointernet,wang2013talking,turkle2011life}) argue
that obfuscation approaches are not just ethical, but necessary, for individuals
to maintain the autonomy and freedom from being entrapped into a single identity
tied to their presence in the physical world.  In particular, it has been argued
(e.g. \cite{wang2013talking}) that, the freedom to have multiple, flexible
identities, including borrowed identities \cite{dalton2013Pseudonymity}, group
identities, and role-based identities, foster the kinds of identity
experimentation that leads people to learn how to cultivate healthy social
relationships throughout their lives.  This principle, known as the ``elastic
self'', has been responsible for key growth in online communities to date, and
is what is being threatened by the surveillance and real-name policies mentioned
previously. 

The approaches outlined here, in concert with other tools to aid privacy,
anonymity and ambiguity in communications, help to redress the current
power imbalance and reduce the effects of caustic surveillance. By adding some
elasticity to the system, uses regain some autonomy in the way they are seen
and sorted online, and develop power over the constitution of their online
identity.

\section{Conclusion}

In this chapter, we have presented a particular approach to shifting the
boundary between privacy and use of data. We have taken a purposefully
adversarial approach to the protection of personal data as an indication of
potential steps which can be taken by individuals at a grass-roots level. 
These techniques are not the final word in personal data sharing, but sketch out
a particular position in the journey towards a balanced societal attitude
towards surveillance.
In time we hope that stronger legal and social protections of personal data will be
introduced, so that society can enjoy the benefits of data sharing while being
respectful of the individual's right to 
privacy~\cite{rooney2012OpenData,rauhofer2012FutureProofing}. 



%%%%%%%%%%% The bibliography starts:
%\begin{thebibliography}{99}
%\bibitem{r1}
%\end{thebibliography}

\bibliographystyle{abbrv}
\bibliography{palimpsests}


\end{document}
